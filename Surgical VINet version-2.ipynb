{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from utils import tools\n",
    "from sklearn import preprocessing\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "\n",
    "# import FlowNetC\n",
    "from networks import FlowNetC\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import flowlib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tqdm\n",
    "import cv2\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('data/caolin_foot_trial1-4.mp4')\n",
    "name = 'caolin'\n",
    "\n",
    "framerate = cap.get(5)\n",
    "total_image = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f'The oringinal framerate is {cap.get(5)} with frame resolution of: {cap.get(3), cap.get(4)}')\n",
    "print(f'The total number of frame in this video is {total_image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "while(cap.isOpened()):\n",
    "    frameID = cap.get(1) # get the current frame number\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(ret != True):\n",
    "        print(f'We\\'ve gotten {int(frameID/5)+1} frames from this video.')\n",
    "        break\n",
    "    # Take at 5Hz frequency which is framerate/5\n",
    "    if (frameID % int(framerate/5) == 0):\n",
    "        frame = frame[64:, 170:600 , :]\n",
    "        filename = 'Train_1/'+ name + \"_frame%04d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUilding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \n",
    "    def __init__(self, num_train_img = 3400, base_dir = '/home/projectx/Documents/GitHub repos/VINET_modification/', sequence = 'Train_1/'):  # base_dir(image) and sequence(lstm) are directories\n",
    "        self.base_dir = base_dir\n",
    "        self.sequence = sequence\n",
    "        self.base_path_img = self.base_dir + self.sequence\n",
    "        \n",
    "        self.image_files = os.listdir(self.base_path_img)\n",
    "        self.image_files.sort()\n",
    "        self.image_files = self.image_files[0:num_train_img]\n",
    "        \n",
    "        # normalization for lstm data\n",
    "        self.train_scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "        ## Omega.7 and load cells\n",
    "        self.input_lstm = self.read_OMEGA7_LC()\n",
    "        \n",
    "        \n",
    "        self.imu_seq_len = 20\n",
    "    \n",
    "    def read_OMEGA7_LC(self, path='data/result_1.csv'):\n",
    "        # read csv data\n",
    "        df = pd.read_csv(path,header = None)\n",
    "        df = df[:874300]\n",
    "        \n",
    "        # take moving average of every 10 data points\n",
    "        new_df = df.groupby(df.index//10).mean()\n",
    "        array_input = np.array(new_df)\n",
    "        \n",
    "        # normalization\n",
    "        array_input_scaled = self.train_scaler.fit_transform(array_input)\n",
    "        \n",
    "        input_lstm  = Variable(torch.from_numpy(array_input_scaled).type(torch.FloatTensor))\n",
    "        input_lstm = input_lstm[:80000,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape to (num_dataset, sequence_length, feature_size)\n",
    "        input_lstm = input_lstm.view(-1,20,11)\n",
    "        \n",
    "        return np.array(input_lstm)\n",
    "    \n",
    "    \n",
    "    def get_input_lstm(self):\n",
    "        return self.input_lstm\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def load_img_bat(self, idx, batch):\n",
    "        batch_x = []\n",
    "        batch_input_lstm = []\n",
    "        for i in range(batch):\n",
    "            x_data_np_1 = np.array(Image.open(self.base_path_img + self.image_files[idx + i]))\n",
    "            x_data_np_2 = np.array(Image.open(self.base_path_img + self.image_files[idx+1 + i]))\n",
    "            x_data_np_1 = x_data_np_1.reshape(3,512,430)\n",
    "            x_data_np_2 = x_data_np_2.reshape(3,512,430)\n",
    "\n",
    "#             ## 3 channels\n",
    "#             x_data_np_1 = np.array([x_data_np_1, x_data_np_1, x_data_np_1])\n",
    "#             x_data_np_2 = np.array([x_data_np_2, x_data_np_2, x_data_np_2])\n",
    "\n",
    "            X = np.array([x_data_np_1, x_data_np_2])\n",
    "            batch_x.append(X)\n",
    "        \n",
    "#           self.input_lstm of size: (num_dataset, sequence_length, feature_size)\n",
    "            tmp = np.array(self.input_lstm[idx + i])\n",
    "            batch_input_lstm.append(tmp)\n",
    "            \n",
    "        \n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_input_lstm = np.array(batch_input_lstm)\n",
    "        \n",
    "        X = Variable(torch.from_numpy(batch_x).type(torch.FloatTensor).cuda())    \n",
    "        X2 = Variable(torch.from_numpy(batch_input_lstm).type(torch.FloatTensor).cuda())    \n",
    "        \n",
    "        Y = X2[:,:,-3:]\n",
    "        \n",
    "     \n",
    "        return X, X2 , Y.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 20, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_data = dataset.get_input_lstm()\n",
    "# Though there are 4000, we will only call first 3400, constrained by the length of 'num_train_img'\n",
    "lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_dim(lstm_input):\n",
    "    fig, axs = plt.subplots(4,3,figsize=(15,15))\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            if (i == 3 and j == 2):\n",
    "                break\n",
    "\n",
    "            x = np.arange(0,80000)\n",
    "            axs[i,j].plot(x,lstm_input[:,i*3+j])\n",
    "            axs[i,j].set_title(f'Feature {i*3+j+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM input data is all normalized in 11-channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = dataset.read_OMEGA7_LC()\n",
    "lstm_input = lstm_input.reshape(4000*20, -1)\n",
    "plot_feature_dim(lstm_input)\n",
    "lstm_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vinet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vinet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=57464,#49152,#24576, \n",
    "            hidden_size=1024,#64, \n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnn.cuda()\n",
    "        \n",
    "        self.rnnIMU = nn.LSTM(\n",
    "            input_size=11, \n",
    "            hidden_size=6,\n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnnIMU.cuda()\n",
    "        \n",
    "        self.linear1 = nn.Linear(1024, 128)\n",
    "        self.linear2 = nn.Linear(128, 3)\n",
    "        #self.linear3 = nn.Linear(128, 6)\n",
    "        self.linear1.cuda()\n",
    "        self.linear2.cuda()\n",
    "        #self.linear3.cuda()\n",
    "        \n",
    "        # load checkpoint state from NVIDIA training\n",
    "        checkpoint_pytorch = 'None'\n",
    "        checkpoint_pytorch = 'FlowNet2-C_checkpoint.pth.tar'\n",
    "        #checkpoint_pytorch = '/notebooks/data/model/FlowNet2-SD_checkpoint.pth.tar'\n",
    "        if os.path.isfile(checkpoint_pytorch):\n",
    "            print('pre_trained_weights found')\n",
    "            checkpoint = torch.load(checkpoint_pytorch,\\\n",
    "                                map_location=lambda storage, loc: storage.cuda(0))\n",
    "            pop_list = [\"deconv5.0.weight\", \"deconv5.0.bias\", \"deconv4.0.weight\", \"deconv4.0.bias\", \"deconv3.0.weight\", \"deconv3.0.bias\", \"deconv2.0.weight\", \"deconv2.0.bias\", \"predict_flow6.weight\", \"predict_flow6.bias\", \"predict_flow5.weight\", \"predict_flow5.bias\", \"predict_flow4.weight\", \"predict_flow4.bias\", \"predict_flow3.weight\", \"predict_flow3.bias\", \"predict_flow2.weight\", \"predict_flow2.bias\", \"upsampled_flow6_to_5.weight\", \"upsampled_flow6_to_5.bias\", \"upsampled_flow5_to_4.weight\", \"upsampled_flow5_to_4.bias\", \"upsampled_flow4_to_3.weight\", \"upsampled_flow4_to_3.bias\", \"upsampled_flow3_to_2.weight\", \"upsampled_flow3_to_2.bias\"]\n",
    "            for name in pop_list:\n",
    "                checkpoint['state_dict'].pop(name);\n",
    "        \n",
    "        self.flownet_c = FlowNetC.FlowNetC(batchNorm=False)\n",
    "        print('....creating model....')\n",
    "        self.flownet_c.load_state_dict(checkpoint['state_dict'])\n",
    "        print('....loading weights....')\n",
    "        self.flownet_c.cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, image, imu):\n",
    "        batch_size, timesteps, C, H, W = image.size()\n",
    "        \n",
    "        ## Input1: Feed image pairs to FlownetC\n",
    "        c_in = image.view(batch_size, timesteps * C, H, W)\n",
    "        c_out = self.flownet_c(c_in)\n",
    "#         print('c_out', c_out.shape)\n",
    "        \n",
    "        ## Input2: Feed IMU records to LSTM\n",
    "        imu_out, (imu_n, imu_c) = self.rnnIMU(imu)\n",
    "        # to match Vision output shape\n",
    "        imu_out = imu_out.view(batch_size,1,-1)   # (batch_size, 1, total_hidden_size)\n",
    "        \n",
    "#         print('imu_out', imu_out.shape)\n",
    "        \n",
    "        \n",
    "        ## Combine the output of input1 and 2 and feed it to LSTM\n",
    "        #r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_in = c_out.view(batch_size, 1, -1)\n",
    "#         print('r_in', r_in.shape)\n",
    "        \n",
    "\n",
    "        cat_out = torch.cat((r_in, imu_out), 2)#1 1 49158\n",
    "#         print(cat_out.shape)\n",
    "        \n",
    "        \n",
    "        r_out, (h_n, h_c) = self.rnn(cat_out)  # (1, 1, 1024)\n",
    "#         print('r_out', r_out.shape)\n",
    "        l_out1 = self.linear1(r_out[:,-1,:])\n",
    "        l_out2 = self.linear2(l_out1)\n",
    "        \n",
    "#         print('r_ol_out2ut', l_out2.shape)\n",
    "        #l_out3 = self.linear3(l_out2)\n",
    "\n",
    "        return l_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_trained_weights found\n",
      "....creating model....\n",
      "....loading weights....\n"
     ]
    }
   ],
   "source": [
    "model = Vinet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
