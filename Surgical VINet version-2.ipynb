{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from utils import tools\n",
    "from sklearn import preprocessing\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "\n",
    "# import FlowNetC\n",
    "from networks import FlowNetC\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import flowlib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tqdm\n",
    "import cv2\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('data/caolin_foot_trial1-4.mp4')\n",
    "name = 'caolin'\n",
    "\n",
    "framerate = cap.get(5)\n",
    "total_image = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f'The oringinal framerate is {cap.get(5)} with frame resolution of: {cap.get(3), cap.get(4)}')\n",
    "print(f'The total number of frame in this video is {total_image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "while(cap.isOpened()):\n",
    "    frameID = cap.get(1) # get the current frame number\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(ret != True):\n",
    "        print(f'We\\'ve gotten {int(frameID/5)+1} frames from this video.')\n",
    "        break\n",
    "    # Take at 5Hz frequency which is framerate/5\n",
    "    if (frameID % int(framerate/5) == 0):\n",
    "        frame = frame[64:, 170:600 , :]\n",
    "        filename = 'Train_1/'+ name + \"_frame%04d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUilding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \n",
    "    def __init__(self, num_train_img = 3400, base_dir = '/home/projectx/Documents/GitHub repos/VINET_modification/', sequence = 'Train_1/'):  # base_dir(image) and sequence(lstm) are directories\n",
    "        self.base_dir = base_dir\n",
    "        self.sequence = sequence\n",
    "        self.base_path_img = self.base_dir + self.sequence\n",
    "        \n",
    "        self.image_files = os.listdir(self.base_path_img)\n",
    "        self.image_files.sort()\n",
    "        self.image_files = self.image_files[0:num_train_img]\n",
    "        \n",
    "        # normalization for lstm data\n",
    "        self.train_scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "        ## Omega.7 and load cells\n",
    "        self.input_lstm = self.read_OMEGA7_LC()\n",
    "        \n",
    "        \n",
    "        self.imu_seq_len = 20\n",
    "    \n",
    "    def read_OMEGA7_LC(self, path='data/result_1.csv'):\n",
    "        # read csv data\n",
    "        df = pd.read_csv(path,header = None)\n",
    "        df = df[:874300]\n",
    "        \n",
    "        # take moving average of every 10 data points\n",
    "        new_df = df.groupby(df.index//10).mean()\n",
    "        array_input = np.array(new_df)\n",
    "        \n",
    "        # normalization\n",
    "        array_input_scaled = self.train_scaler.fit_transform(array_input)\n",
    "        \n",
    "        input_lstm  = Variable(torch.from_numpy(array_input_scaled).type(torch.FloatTensor))\n",
    "        input_lstm = input_lstm[:80000,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape to (num_dataset, sequence_length, feature_size)\n",
    "        input_lstm = input_lstm.view(-1,20,11)\n",
    "        \n",
    "        return np.array(input_lstm)\n",
    "    \n",
    "    \n",
    "    def get_input_lstm(self):\n",
    "        return self.input_lstm\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def load_img_bat(self, idx, batch):\n",
    "        batch_x = []\n",
    "        batch_input_lstm = []\n",
    "        for i in range(batch):\n",
    "            x_data_np_1 = np.array(Image.open(self.base_path_img + self.image_files[idx + i]))\n",
    "            x_data_np_2 = np.array(Image.open(self.base_path_img + self.image_files[idx+1 + i]))\n",
    "            x_data_np_1 = x_data_np_1.reshape(3,512,430)\n",
    "            x_data_np_2 = x_data_np_2.reshape(3,512,430)\n",
    "\n",
    "#             ## 3 channels\n",
    "#             x_data_np_1 = np.array([x_data_np_1, x_data_np_1, x_data_np_1])\n",
    "#             x_data_np_2 = np.array([x_data_np_2, x_data_np_2, x_data_np_2])\n",
    "\n",
    "            X = np.array([x_data_np_1, x_data_np_2])\n",
    "            batch_x.append(X)\n",
    "        \n",
    "#           self.input_lstm of size: (num_dataset, sequence_length, feature_size)\n",
    "            tmp = np.array(self.input_lstm[idx + i])\n",
    "            batch_input_lstm.append(tmp)\n",
    "            \n",
    "        \n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_input_lstm = np.array(batch_input_lstm)\n",
    "        \n",
    "        X = Variable(torch.from_numpy(batch_x).type(torch.FloatTensor).cuda())    \n",
    "        X2 = Variable(torch.from_numpy(batch_input_lstm).type(torch.FloatTensor).cuda())    \n",
    "        \n",
    "        Y = X2[:,:,-3:]\n",
    "        \n",
    "     \n",
    "        return X, X2 , Y.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data = dataset.get_input_lstm()\n",
    "# Though there are 4000, we will only call first 3400, constrained by the length of 'num_train_img'\n",
    "lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_dim(lstm_input):\n",
    "    fig, axs = plt.subplots(4,3,figsize=(15,15))\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            if (i == 3 and j == 2):\n",
    "                break\n",
    "\n",
    "            x = np.arange(0,80000)\n",
    "            axs[i,j].plot(x,lstm_input[:,i*3+j])\n",
    "            axs[i,j].set_title(f'Feature {i*3+j+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM input data is all normalized in 11-channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = dataset.read_OMEGA7_LC()\n",
    "lstm_input = lstm_input.reshape(4000*20, -1)\n",
    "plot_feature_dim(lstm_input)\n",
    "lstm_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Surgical_VINet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Surgical_VINet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=118 ,#49152,#24576, \n",
    "            hidden_size=256,#64, \n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnn.cuda()\n",
    "        \n",
    "        self.rnnIMU = nn.LSTM(\n",
    "            input_size=11, \n",
    "            hidden_size=6,\n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnnIMU.cuda()\n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 128)\n",
    "        self.linear2 = nn.Linear(128, 3)\n",
    "        #self.linear3 = nn.Linear(128, 6)\n",
    "        self.linear1.cuda()\n",
    "        self.linear2.cuda()\n",
    "        #self.linear3.cuda()\n",
    "        \n",
    "        # load checkpoint state from NVIDIA training\n",
    "        checkpoint_pytorch = 'None'\n",
    "        checkpoint_pytorch = 'FlowNet2-C_checkpoint.pth.tar'\n",
    "        #checkpoint_pytorch = '/notebooks/data/model/FlowNet2-SD_checkpoint.pth.tar'\n",
    "        if os.path.isfile(checkpoint_pytorch):\n",
    "            print('pre_trained_weights found')\n",
    "            checkpoint = torch.load(checkpoint_pytorch,\\\n",
    "                                map_location=lambda storage, loc: storage.cuda(0))\n",
    "            pop_list = [\"deconv5.0.weight\", \"deconv5.0.bias\", \"deconv4.0.weight\", \"deconv4.0.bias\", \"deconv3.0.weight\", \"deconv3.0.bias\", \"deconv2.0.weight\", \"deconv2.0.bias\",  \"predict_flow5.weight\", \"predict_flow5.bias\", \"predict_flow4.weight\", \"predict_flow4.bias\", \"predict_flow3.weight\", \"predict_flow3.bias\", \"predict_flow2.weight\", \"predict_flow2.bias\", \"upsampled_flow6_to_5.weight\", \"upsampled_flow6_to_5.bias\", \"upsampled_flow5_to_4.weight\", \"upsampled_flow5_to_4.bias\", \"upsampled_flow4_to_3.weight\", \"upsampled_flow4_to_3.bias\", \"upsampled_flow3_to_2.weight\", \"upsampled_flow3_to_2.bias\"]\n",
    "            for name in pop_list:\n",
    "                checkpoint['state_dict'].pop(name);\n",
    "        \n",
    "        self.flownet_c = FlowNetC.FlowNetC(batchNorm=False)\n",
    "        print('....creating model....')\n",
    "        self.flownet_c.load_state_dict(checkpoint['state_dict'])\n",
    "        print('....loading weights....')\n",
    "        self.flownet_c.cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, image, imu):\n",
    "        batch_size, timesteps, C, H, W = image.size()\n",
    "        \n",
    "        ## Input1: Feed image pairs to FlownetC\n",
    "        c_in = image.view(batch_size, timesteps * C, H, W)\n",
    "        c_out = self.flownet_c(c_in) # [1, 2, 8, 7]\n",
    "#         print('c_out', c_out.shape)\n",
    "        \n",
    "        ## Input2: Feed IMU records to LSTM\n",
    "        imu_out, (imu_n, imu_c) = self.rnnIMU(imu)\n",
    "        # to match Vision output shape\n",
    "        imu_out = imu_out.view(batch_size,1,-1)   # (batch_size, 1, total_hidden_size) [1, 1, 6]\n",
    "        \n",
    "#         print('imu_out', imu_out.shape)\n",
    "        \n",
    "        \n",
    "        ## Combine the output of input1 and 2 and feed it to LSTM\n",
    "        #r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_in = c_out.view(batch_size, 1, -1) # [1,1,112]\n",
    "#         print('r_in', r_in.shape)\n",
    "        \n",
    "\n",
    "        cat_out = torch.cat((r_in, imu_out), 2)#[1 1 118]\n",
    "#         print(cat_out.shape)\n",
    "        \n",
    "        \n",
    "        r_out, (h_n, h_c) = self.rnn(cat_out)  # (1, 1, 256)\n",
    "#         print('r_out', r_out.shape)\n",
    "        l_out1 = self.linear1(r_out[:,-1,:])\n",
    "        l_out2 = self.linear2(l_out1)\n",
    "        \n",
    "#         print('r_ol_out2ut', l_out2.shape)\n",
    "        #l_out3 = self.linear3(l_out2)\n",
    "\n",
    "        return l_out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Our model & Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_trained_weights found\n",
      "....creating model....\n",
      "....loading weights....\n"
     ]
    }
   ],
   "source": [
    "model = Surgical_VINet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 512, 430]), torch.Size([1, 20, 11]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "X, X2 , Y = dataset.load_img_bat(0,1)\n",
    "X.size(), X2.size() , Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 118, got 232",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7169ea0d6517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-10fb87e81d94>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, imu)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1, 1, 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;31m#         print('r_out', r_out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0ml_out1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[Tensor, Tensor], Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    147\u001b[0m             raise RuntimeError(\n\u001b[1;32m    148\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 149\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 118, got 232"
     ]
    }
   ],
   "source": [
    "output = model(X,X2)\n",
    "(output.size(), Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss(size_average=False)\n",
    "\n",
    "loss = criterion(output, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulization of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = X\n",
    "imu   = X2\n",
    "y = model(image, imu)\n",
    "dot = make_dot(y, params = dict(list(model.named_parameters()) + [('x',X), ('x2',X2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot.save(filename='My_new_Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # hyper-parameters\n",
    "    epoch = 2\n",
    "    batch = 1\n",
    "    lr = 0.001\n",
    "    \n",
    "    # creat model and dataset\n",
    "    model = Surgical_VINet()\n",
    "    mydataset = MyDataset()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # TensorBoard\n",
    "    writer = SummaryWriter()\n",
    " \n",
    "    model.train()\n",
    "    \n",
    "    mydataset = MyDataset()\n",
    "    \n",
    "#     criterion = nn.MSELoss()\n",
    "    criterion  = nn.L1Loss(size_average=False)\n",
    "    \n",
    "    start = 5\n",
    "    end = len(mydataset) - batch\n",
    "    batch_num = (end - start)\n",
    "    startT = time.time()\n",
    "    \n",
    "    with tools.TimerBlock(\"Start training\") as block :\n",
    "        for k in range(epoch):\n",
    "            for i in range(start, end):\n",
    "                data, data_lstm, target = mydataset.load_img_bat(i, batch)\n",
    "#                 data, data_lstm, target = data.cuda(), data_lstm.cuda(), target.cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward pass\n",
    "                output = model(data, data_lstm)\n",
    "                \n",
    "                # compute loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_value_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                avgTime = block.avg()\n",
    "                remainingTime = int((batch_num*epoch -  (i + batch_num*k)) * avgTime)\n",
    "                rTime_str = \"{:02d}:{:02d}:{:02d}\".format(int(remainingTime/60//60), \n",
    "                                                          int(remainingTime//60%60), \n",
    "                                                          int(remainingTime%60))\n",
    "                block.log(f'Train Epoch: {k}\\t[{i}/{batch_num} ({(100.*(i + batch_num*k)):.0f}%)]\\tLoss: {loss.data:.6f}, TimeAvg: {avgTime:.4f}, Remaining: {rTime_str}')\n",
    "                          \n",
    "                writer.add_scalar('loss', loss.data, k*batch_num + i)\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "            check_str = 'checkpoint_{}.pt'.format(k)\n",
    "            if (k+1)%5 == 0:\n",
    "                torch.save(model.state_dict(), check_str)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'test_network_trial_1.pt')\n",
    "    writer.export_scalars_to_json(\"./summary_writer.json\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
