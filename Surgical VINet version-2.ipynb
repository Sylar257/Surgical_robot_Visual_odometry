{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from utils import tools\n",
    "from sklearn import preprocessing\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "\n",
    "# import FlowNetC\n",
    "from networks import FlowNetC\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import flowlib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tqdm\n",
    "import cv2\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The oringinal framerate is 25.0 with frame resolution of: (768.0, 576.0)\n",
      "The total number of frame in this video is 17155\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('data/caolin_foot_trial1-4.mp4')\n",
    "name = 'caolin'\n",
    "\n",
    "framerate = cap.get(5)\n",
    "total_image = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f'The oringinal framerate is {cap.get(5)} with frame resolution of: {cap.get(3), cap.get(4)}')\n",
    "print(f'The total number of frame in this video is {total_image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've gotten 3432 frames from this video.\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "while(cap.isOpened()):\n",
    "    frameID = cap.get(1) # get the current frame number\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(ret != True):\n",
    "        print(f'We\\'ve gotten {int(frameID/5)+1} frames from this video.')\n",
    "        break\n",
    "    # Take at 5Hz frequency which is framerate/5\n",
    "    if (frameID % int(framerate/5) == 0):\n",
    "        frame = frame[64:, 162:610 , :]\n",
    "        filename = 'Train_1/'+ name + \"_frame%04d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUilding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \n",
    "    def __init__(self, num_train_img = 3400, base_dir = '/home/projectx/Documents/GitHub repos/VINET_modification/', sequence = 'Train_1/'):  # base_dir(image) and sequence(lstm) are directories\n",
    "        self.base_dir = base_dir\n",
    "        self.sequence = sequence\n",
    "        self.base_path_img = self.base_dir + self.sequence\n",
    "        \n",
    "        self.image_files = os.listdir(self.base_path_img)\n",
    "        self.image_files.sort()\n",
    "        self.image_files = self.image_files[0:num_train_img]\n",
    "        \n",
    "        # normalization for lstm data\n",
    "        self.train_scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "        ## Omega.7 and load cells\n",
    "        self.input_lstm = self.read_OMEGA7_LC()\n",
    "        \n",
    "        \n",
    "        self.imu_seq_len = 20\n",
    "    \n",
    "    def read_OMEGA7_LC(self, path='data/result_1.csv'):\n",
    "        # read csv data\n",
    "        df = pd.read_csv(path,header = None)\n",
    "        df = df[:874300]\n",
    "        \n",
    "        # take moving average of every 10 data points\n",
    "        new_df = df.groupby(df.index//10).mean()\n",
    "        array_input = np.array(new_df)\n",
    "        \n",
    "        # normalization\n",
    "        array_input_scaled = self.train_scaler.fit_transform(array_input)\n",
    "        \n",
    "        input_lstm  = Variable(torch.from_numpy(array_input_scaled).type(torch.FloatTensor))\n",
    "        input_lstm = input_lstm[:80000,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape to (num_dataset, sequence_length, feature_size)\n",
    "        input_lstm = input_lstm.view(-1,20,11)\n",
    "        \n",
    "        return np.array(input_lstm)\n",
    "    \n",
    "    \n",
    "    def get_input_lstm(self):\n",
    "        return self.input_lstm\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def load_img_bat(self, idx, batch):\n",
    "        batch_x = []\n",
    "        batch_input_lstm = []\n",
    "        for i in range(batch):\n",
    "            x_data_np_1 = np.array(Image.open(self.base_path_img + self.image_files[idx + i]))\n",
    "            x_data_np_2 = np.array(Image.open(self.base_path_img + self.image_files[idx+1 + i]))\n",
    "            x_data_np_1 = x_data_np_1.reshape(3,512,430)\n",
    "            x_data_np_2 = x_data_np_2.reshape(3,512,430)\n",
    "\n",
    "#             ## 3 channels\n",
    "#             x_data_np_1 = np.array([x_data_np_1, x_data_np_1, x_data_np_1])\n",
    "#             x_data_np_2 = np.array([x_data_np_2, x_data_np_2, x_data_np_2])\n",
    "\n",
    "            X = np.array([x_data_np_1, x_data_np_2])\n",
    "            batch_x.append(X)\n",
    "        \n",
    "#           self.input_lstm of size: (num_dataset, sequence_length, feature_size)\n",
    "            tmp = np.array(self.input_lstm[idx + i])\n",
    "            batch_input_lstm.append(tmp)\n",
    "            \n",
    "        \n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_input_lstm = np.array(batch_input_lstm)\n",
    "        \n",
    "        X = Variable(torch.from_numpy(batch_x).type(torch.FloatTensor).cuda())    \n",
    "        X2 = Variable(torch.from_numpy(batch_input_lstm).type(torch.FloatTensor).cuda())    \n",
    "        \n",
    "        Y = X2[:,:,-3:]\n",
    "        \n",
    "     \n",
    "        return X, X2 , Y.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 512, 430]), torch.Size([1, 20, 11]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "X, X2 , Y = dataset.load_img_bat(0,1)\n",
    "X.size(), X2.size() , Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 512, 430]), torch.Size([1, 20, 11]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "X, X2 , Y = dataset.load_img_bat(0,1)\n",
    "X.size(), X2.size() , Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data = dataset.get_input_lstm()\n",
    "# Though there are 4000, we will only call first 3400, constrained by the length of 'num_train_img'\n",
    "lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_dim(lstm_input):\n",
    "    fig, axs = plt.subplots(4,3,figsize=(15,15))\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            if (i == 3 and j == 2):\n",
    "                break\n",
    "\n",
    "            x = np.arange(0,80000)\n",
    "            axs[i,j].plot(x,lstm_input[:,i*3+j])\n",
    "            axs[i,j].set_title(f'Feature {i*3+j+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM input data is all normalized in 11-channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = dataset.read_OMEGA7_LC()\n",
    "lstm_input = lstm_input.reshape(4000*20, -1)\n",
    "plot_feature_dim(lstm_input)\n",
    "lstm_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Surgical_VINet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Surgical_VINet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=232 ,#49152,#24576, \n",
    "            hidden_size=256,#64, \n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnn.cuda()\n",
    "        \n",
    "        self.rnnIMU = nn.LSTM(\n",
    "            input_size=11, \n",
    "            hidden_size=6,\n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnnIMU.cuda()\n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 128)\n",
    "        self.linear2 = nn.Linear(128, 3)\n",
    "        #self.linear3 = nn.Linear(128, 6)\n",
    "        self.linear1.cuda()\n",
    "        self.linear2.cuda()\n",
    "        #self.linear3.cuda()\n",
    "        \n",
    "        # load checkpoint state from NVIDIA training\n",
    "        checkpoint_pytorch = 'None'\n",
    "        checkpoint_pytorch = 'FlowNet2-C_checkpoint.pth.tar'\n",
    "        #checkpoint_pytorch = '/notebooks/data/model/FlowNet2-SD_checkpoint.pth.tar'\n",
    "        if os.path.isfile(checkpoint_pytorch):\n",
    "            print('pre_trained_weights found')\n",
    "            checkpoint = torch.load(checkpoint_pytorch,\\\n",
    "                                map_location=lambda storage, loc: storage.cuda(0))\n",
    "            pop_list = [\"deconv5.0.weight\", \"deconv5.0.bias\", \"deconv4.0.weight\", \"deconv4.0.bias\", \"deconv3.0.weight\", \"deconv3.0.bias\", \"deconv2.0.weight\", \"deconv2.0.bias\",  \"predict_flow5.weight\", \"predict_flow5.bias\", \"predict_flow4.weight\", \"predict_flow4.bias\", \"predict_flow3.weight\", \"predict_flow3.bias\", \"predict_flow2.weight\", \"predict_flow2.bias\", \"upsampled_flow6_to_5.weight\", \"upsampled_flow6_to_5.bias\", \"upsampled_flow5_to_4.weight\", \"upsampled_flow5_to_4.bias\", \"upsampled_flow4_to_3.weight\", \"upsampled_flow4_to_3.bias\", \"upsampled_flow3_to_2.weight\", \"upsampled_flow3_to_2.bias\"]\n",
    "            for name in pop_list:\n",
    "                checkpoint['state_dict'].pop(name);\n",
    "        \n",
    "        self.flownet_c = FlowNetC.FlowNetC(batchNorm=False)\n",
    "        print('....creating model....')\n",
    "        self.flownet_c.load_state_dict(checkpoint['state_dict'])\n",
    "        print('....loading weights....')\n",
    "        self.flownet_c.cuda()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, image, imu):\n",
    "        batch_size, timesteps, C, H, W = image.size()\n",
    "        \n",
    "        ## Input1: Feed image pairs to FlownetC\n",
    "        c_in = image.view(batch_size, timesteps * C, H, W)\n",
    "        c_out = self.flownet_c(c_in) # [1, 2, 8, 7]\n",
    "#         print('c_out', c_out.shape)\n",
    "        \n",
    "        ## Input2: Feed IMU records to LSTM\n",
    "        imu_out, (imu_n, imu_c) = self.rnnIMU(imu)\n",
    "        # to match Vision output shape\n",
    "        imu_out = imu_out.view(batch_size,1,-1)   # (batch_size, 1, total_hidden_size) [1, 1, 120]\n",
    "        \n",
    "#         print('imu_out', imu_out.shape)\n",
    "        \n",
    "        \n",
    "        ## Combine the output of input1 and 2 and feed it to LSTM\n",
    "        #r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_in = c_out.view(batch_size, 1, -1) # [1,1,112]\n",
    "#         print('r_in', r_in.shape)\n",
    "        \n",
    "\n",
    "        cat_out = torch.cat((r_in, imu_out), 2)#[1 1 232]\n",
    "#         print(cat_out.shape)\n",
    "        \n",
    "        \n",
    "        r_out, (h_n, h_c) = self.rnn(cat_out)  # (1, 1, 256)\n",
    "#         print('r_out', r_out.shape)\n",
    "        l_out1 = self.linear1(r_out[:,-1,:])\n",
    "        l_out2 = self.linear2(l_out1)\n",
    "        \n",
    "#         print('r_ol_out2ut', l_out2.shape)\n",
    "        #l_out3 = self.linear3(l_out2)\n",
    "\n",
    "        return l_out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Our model & Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_trained_weights found\n",
      "....creating model....\n",
      "....loading weights....\n"
     ]
    }
   ],
   "source": [
    "model = Surgical_VINet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 512, 430]), torch.Size([1, 20, 11]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "X, X2 , Y = dataset.load_img_bat(0,1)\n",
    "X.size(), X2.size() , Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    }
   ],
   "source": [
    "batch_size, timesteps, C, H, W = X.size()\n",
    "        \n",
    "## Input1: Feed image pairs to FlownetC\n",
    "c_in = X.view(batch_size, timesteps * C, H, W)\n",
    "c_out = model.flownet_c(c_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_out_to_flow_png(output):\n",
    "    out_np = output[0].data.cpu().numpy()\n",
    "\n",
    "    #https://gitorchub.com/DediGadot/PatchBatch/blob/master/flowlib.py\n",
    "    out_np = np.squeeze(out_np)\n",
    "    out_np = np.moveaxis(out_np,0, -1)\n",
    "\n",
    "    im_arr = flowlib.flow_to_image(out_np)\n",
    "    im = Image.fromarray(im_arr)\n",
    "    im.save('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max flow: 11038.9512\n",
      "flow range:\n",
      "u = -10953.464 .. 2095.344\n",
      "v = -4647.612 .. 3800.490\n"
     ]
    }
   ],
   "source": [
    "import flowlib\n",
    "model_out_to_flow_png(c_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(X,X2)\n",
    "(output.size(), Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8847, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.0458, device='cuda:0', grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.L1Loss(size_average=False)\n",
    "\n",
    "loss = criterion(output, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulization of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = X\n",
    "imu   = X2\n",
    "y = model(image, imu)\n",
    "dot = make_dot(y, params = dict(list(model.named_parameters()) + [('x',X), ('x2',X2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot.save(filename='My_new_Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # hyper-parameters\n",
    "    epoch = 2\n",
    "    batch = 1\n",
    "    lr = 0.001\n",
    "    \n",
    "    # creat model and dataset\n",
    "    model = Surgical_VINet()\n",
    "    mydataset = MyDataset()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # TensorBoard\n",
    "    writer = SummaryWriter()\n",
    " \n",
    "    model.train()\n",
    "    \n",
    "    mydataset = MyDataset()\n",
    "    \n",
    "#     criterion = nn.MSELoss()\n",
    "    criterion  = nn.L1Loss(size_average=False)\n",
    "    \n",
    "    start = 5\n",
    "    end = len(mydataset) - batch\n",
    "    batch_num = (end - start)\n",
    "    startT = time.time()\n",
    "    \n",
    "    with tools.TimerBlock(\"Start training\") as block :\n",
    "        for k in range(epoch):\n",
    "            for i in range(start, end):\n",
    "                data, data_lstm, target = mydataset.load_img_bat(i, batch)\n",
    "#                 data, data_lstm, target = data.cuda(), data_lstm.cuda(), target.cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward pass\n",
    "                output = model(data, data_lstm)\n",
    "                \n",
    "                # compute loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_value_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                avgTime = block.avg()\n",
    "                remainingTime = int((batch_num*epoch -  (i + batch_num*k)) * avgTime)\n",
    "                rTime_str = \"{:02d}:{:02d}:{:02d}\".format(int(remainingTime/60//60), \n",
    "                                                          int(remainingTime//60%60), \n",
    "                                                          int(remainingTime%60))\n",
    "                block.log(f'Train Epoch: {k}\\t[{i}/{batch_num} ({(100.*(i + batch_num*k)):.0f}%)]\\tLoss: {loss.data:.6f}, TimeAvg: {avgTime:.4f}, Remaining: {rTime_str}')\n",
    "                          \n",
    "                writer.add_scalar('loss', loss.data, k*batch_num + i)\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "            check_str = 'checkpoint_{}.pt'.format(k)\n",
    "            if (k+1)%5 == 0:\n",
    "                torch.save(model.state_dict(), check_str)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'test_network_trial_1.pt')\n",
    "    writer.export_scalars_to_json(\"./summary_writer.json\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
