{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from utils import tools\n",
    "from sklearn import preprocessing\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "\n",
    "# import FlowNetC\n",
    "from networks import FlowNetC\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import flowlib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tqdm\n",
    "import cv2\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \n",
    "    def __init__(self, num_train_img = 3400, base_dir = '/home/projectx/Documents/GitHub repos/VINET_modification/', sequence = 'Train_1/'):  # base_dir(image) and sequence(lstm) are directories\n",
    "        self.base_dir = base_dir\n",
    "        self.sequence = sequence\n",
    "        self.base_path_img = self.base_dir + self.sequence\n",
    "        \n",
    "        self.image_files = os.listdir(self.base_path_img)\n",
    "        self.image_files.sort()\n",
    "        self.image_files = self.image_files[0:num_train_img]\n",
    "        \n",
    "        # normalization for lstm data\n",
    "        self.train_scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "        ## Omega.7 and load cells\n",
    "        self.input_lstm = self.read_OMEGA7_LC()\n",
    "        \n",
    "        \n",
    "        self.imu_seq_len = 20\n",
    "    \n",
    "    def read_OMEGA7_LC(self, path='data/result_1.csv'):\n",
    "        # read csv data\n",
    "        df = pd.read_csv(path,header = None)\n",
    "        df = df[:874300]\n",
    "        \n",
    "        # take moving average of every 10 data points\n",
    "        new_df = df.groupby(df.index//10).mean()\n",
    "        array_input = np.array(new_df)\n",
    "        \n",
    "        # normalization\n",
    "        array_input_scaled = self.train_scaler.fit_transform(array_input)\n",
    "        \n",
    "        input_lstm  = Variable(torch.from_numpy(array_input_scaled).type(torch.FloatTensor))\n",
    "        input_lstm = input_lstm[:80000,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape to (num_dataset, sequence_length, feature_size)\n",
    "        input_lstm = input_lstm.view(-1,20,11)\n",
    "        \n",
    "        return np.array(input_lstm)\n",
    "    \n",
    "    \n",
    "    def get_input_lstm(self):\n",
    "        return self.input_lstm\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def load_img_bat(self, idx, batch):\n",
    "        batch_x = []\n",
    "        batch_input_lstm = []\n",
    "        for i in range(batch):\n",
    "            x_data_np_1 = np.array(Image.open(self.base_path_img + self.image_files[idx + i]))\n",
    "            x_data_np_2 = np.array(Image.open(self.base_path_img + self.image_files[idx+1 + i]))\n",
    "            x_data_np_1 = x_data_np_1.reshape(3,512,448)\n",
    "            x_data_np_2 = x_data_np_2.reshape(3,512,448)\n",
    "\n",
    "#             ## 3 channels\n",
    "#             x_data_np_1 = np.array([x_data_np_1, x_data_np_1, x_data_np_1])\n",
    "#             x_data_np_2 = np.array([x_data_np_2, x_data_np_2, x_data_np_2])\n",
    "\n",
    "            X = np.array([x_data_np_1, x_data_np_2])\n",
    "            batch_x.append(X)\n",
    "        \n",
    "#           self.input_lstm of size: (num_dataset, sequence_length, feature_size)\n",
    "            tmp = np.array(self.input_lstm[idx + i])\n",
    "            batch_input_lstm.append(tmp)\n",
    "            \n",
    "        \n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_input_lstm = np.array(batch_input_lstm)\n",
    "        \n",
    "        X = Variable(torch.from_numpy(batch_x).type(torch.FloatTensor).cuda())    \n",
    "        X2 = Variable(torch.from_numpy(batch_input_lstm).type(torch.FloatTensor).cuda())    \n",
    "        \n",
    "        Y = X2[:,:,-3:]\n",
    "        \n",
    "     \n",
    "        return X, X2 , Y.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 20, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_data = dataset.get_input_lstm()\n",
    "# Though there are 4000, we will only call first 3400, constrained by the length of 'num_train_img'\n",
    "lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_out_to_flow_png(output):\n",
    "    out_np = output[0].data.cpu().numpy()\n",
    "\n",
    "    #https://gitorchub.com/DediGadot/PatchBatch/blob/master/flowlib.py\n",
    "    out_np = np.squeeze(out_np)\n",
    "    out_np = np.moveaxis(out_np,0, -1)\n",
    "\n",
    "    im_arr = flowlib.flow_to_image(out_np)\n",
    "    im = Image.fromarray(im_arr)\n",
    "    im.save('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from networks.correlation_package.correlation import Correlation\n",
    "from networks.submodules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = FlowNetC.FlowNetC(batchNorm=False)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3325: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3, 512, 448]), torch.Size([1, 20, 11]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "X, X2 , Y = dataset.load_img_bat(0,1)\n",
    "X.size(), X2.size() , Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, timesteps, C, H, W = X.size()\n",
    "img_pair = X.view(batch_size, timesteps * C, H, W)\n",
    "# output = model(img_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 512, 448]),\n",
       " torch.Size([1, 3, 512, 448]),\n",
       " torch.Size([1, 256, 64, 56]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = img_pair[:,0:3,:,:]\n",
    "x2 = img_pair[:,3::,:,:]\n",
    "\n",
    "# for x1\n",
    "out_conv1a = model.conv1(x1)\n",
    "out_conv2a = model.conv2(out_conv1a)\n",
    "out_conv3a = model.conv3(out_conv2a)\n",
    "x1.size(), x2.size(), out_conv3a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 56])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x2\n",
    "out_conv1b = model.conv1(x2)\n",
    "out_conv2b = model.conv2(out_conv1b)\n",
    "out_conv3b = model.conv3(out_conv2b)\n",
    "out_conv3b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 441, 64, 56])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corrilation between x1 and x2\n",
    "out_corr = model.corr(out_conv3a, out_conv3b)\n",
    "out_corr.size()\n",
    "# activate the corrilation\n",
    "out_corr = model.corr_activation(out_corr)\n",
    "out_corr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64, 56])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 473, 64, 56])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redirect top input stream and concatenate the features (addtional 32 features)\n",
    "out_conv_redir = model.conv_redir(out_conv3a)\n",
    "print(out_conv_redir.size())\n",
    "in_conv3_1 = torch.cat((out_conv_redir,out_corr),1)\n",
    "in_conv3_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 56])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge conv layers\n",
    "out_conv3_1 = model.conv3_1(in_conv3_1)\n",
    "out_conv3_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 32, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv4 = model.conv4_1(model.conv4(out_conv3_1))\n",
    "out_conv4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 16, 14])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv5 = model.conv5_1(model.conv5(out_conv4))\n",
    "out_conv5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 8, 7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv6 = model.conv6_1(model.conv6(out_conv5))\n",
    "out_conv6.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 8, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute flow\n",
    "flow6 = model.predict_flow6(out_conv6)\n",
    "flow6.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 16, 14])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow6_up = model.upsampled_flow6_to_5(flow6)\n",
    "flow6_up.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 16, 14])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_deconv5 = model.deconv5(out_conv6)\n",
    "out_deconv5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 16, 14]),\n",
       " torch.Size([1, 512, 16, 14]),\n",
       " torch.Size([1, 2, 16, 14]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv5.size(), out_deconv5.size(), flow6_up.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1026, 16, 14])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n",
    "concat5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 16, 14])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow5 = model.predict_flow5(concat5)\n",
    "flow5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 32, 28])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow5_up = model.upsampled_flow5_to_4(flow5)\n",
    "flow5_up.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 32, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_deconv4 = model.deconv4(concat5)\n",
    "out_deconv4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 32, 28]),\n",
       " torch.Size([1, 256, 32, 28]),\n",
       " torch.Size([1, 2, 32, 28]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv4.size(),out_deconv4.size(),flow5_up.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 770, 32, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n",
    "concat4.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 64, 56])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow4       = model.predict_flow4(concat4)\n",
    "flow4_up    = model.upsampled_flow4_to_3(flow4)\n",
    "out_deconv3 = model.deconv3(concat4)\n",
    "out_deconv3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 64, 56]),\n",
       " torch.Size([1, 128, 64, 56]),\n",
       " torch.Size([1, 2, 64, 56]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv3_1.size(),out_deconv3.size(),flow4_up.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 386, 64, 56])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat3 = torch.cat((out_conv3_1,out_deconv3,flow4_up),1)\n",
    "concat3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 128, 112])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow3       = model.predict_flow3(concat3)\n",
    "flow3_up    = model.upsampled_flow3_to_2(flow3)\n",
    "out_deconv2 = model.deconv2(concat3)\n",
    "out_deconv2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 128, 112]),\n",
       " torch.Size([1, 64, 128, 112]),\n",
       " torch.Size([1, 2, 128, 112]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_conv2a.size(),out_deconv2.size(),flow3_up.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 194, 128, 112])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat2 = torch.cat((out_conv2a,out_deconv2,flow3_up),1)\n",
    "concat2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 128, 112])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow2 = model.predict_flow2(concat2)\n",
    "flow2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 448)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*4, 112*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import FlowNetC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowNetC.FlowNetC(batchNorm=False)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(img_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from networks.Fl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
