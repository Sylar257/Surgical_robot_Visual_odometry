{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from models import FlowNet2\n",
    "from utils.frame_utils import read_gen\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from utils import tools\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argparse():\n",
    "    def __init__(self):\n",
    "        super(argparse, self).__init__()\n",
    "        self.rgb_max = 255.\n",
    "        self.fp16 = False\n",
    "        self.inference_size = [1080, 1920]\n",
    "        self.crop_size = [384, 512]\n",
    "        self.effective_batch_size = 1\n",
    "args = argparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parameter count = 162,518,834'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from networks.resample2d_package.resample2d import Resample2d\n",
    "    from networks.channelnorm_package.channelnorm import ChannelNorm\n",
    "\n",
    "    from networks import FlowNetC\n",
    "    from networks import FlowNetS\n",
    "    from networks import FlowNetSD\n",
    "    from networks import FlowNetFusion\n",
    "\n",
    "    from networks.submodules import *\n",
    "except:\n",
    "    from .networks.resample2d_package.resample2d import Resample2d\n",
    "    from .networks.channelnorm_package.channelnorm import ChannelNorm\n",
    "\n",
    "    from .networks import FlowNetC\n",
    "    from .networks import FlowNetS\n",
    "    from .networks import FlowNetSD\n",
    "    from .networks import FlowNetFusion\n",
    "\n",
    "    from .networks.submodules import *\n",
    "'Parameter count = 162,518,834'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import png\n",
    "import numpy as np\n",
    "import matplotlib.colors as cl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "UNKNOWN_FLOW_THRESH = 1e7\n",
    "SMALLFLOW = 0.0\n",
    "LARGEFLOW = 1e8\n",
    "\n",
    "\"\"\"\n",
    "=============\n",
    "Flow Section\n",
    "=============\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_flow(filename):\n",
    "    \"\"\"\n",
    "    visualize optical flow map using matplotlib\n",
    "    :param filename: optical flow file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    flow = read_flow(filename)\n",
    "    img = flow_to_image(flow)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_flow(flow, mode='Y'):\n",
    "    \"\"\"\n",
    "    this function visualize the input flow\n",
    "    :param flow: input flow in array\n",
    "    :param mode: choose which color mode to visualize the flow (Y: Ccbcr, RGB: RGB color)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if mode == 'Y':\n",
    "        # Ccbcr color wheel\n",
    "        img = flow_to_image(flow)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    elif mode == 'RGB':\n",
    "        (h, w) = flow.shape[0:2]\n",
    "        du = flow[:, :, 0]\n",
    "        dv = flow[:, :, 1]\n",
    "        valid = flow[:, :, 2]\n",
    "        max_flow = max(np.max(du), np.max(dv))\n",
    "        img = np.zeros((h, w, 3), dtype=np.float64)\n",
    "        # angle layer\n",
    "        img[:, :, 0] = np.arctan2(dv, du) / (2 * np.pi)\n",
    "        # magnitude layer, normalized to 1\n",
    "        img[:, :, 1] = np.sqrt(du * du + dv * dv) * 8 / max_flow\n",
    "        # phase layer\n",
    "        img[:, :, 2] = 8 - img[:, :, 1]\n",
    "        # clip to [0,1]\n",
    "        small_idx = img[:, :, 0:3] < 0\n",
    "        large_idx = img[:, :, 0:3] > 1\n",
    "        img[small_idx] = 0\n",
    "        img[large_idx] = 1\n",
    "        # convert to rgb\n",
    "        img = cl.hsv_to_rgb(img)\n",
    "        # remove invalid point\n",
    "        img[:, :, 0] = img[:, :, 0] * valid\n",
    "        img[:, :, 1] = img[:, :, 1] * valid\n",
    "        img[:, :, 2] = img[:, :, 2] * valid\n",
    "        # show\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_flow(filename):\n",
    "    \"\"\"\n",
    "    read optical flow from Middlebury .flo file\n",
    "    :param filename: name of the flow file\n",
    "    :return: optical flow data in matrix\n",
    "    \"\"\"\n",
    "    f = open(filename, 'rb')\n",
    "    magic = np.fromfile(f, np.float32, count=1)\n",
    "    data2d = None\n",
    "\n",
    "    if 202021.25 != magic:\n",
    "        print('Magic number incorrect. Invalid .flo file')\n",
    "    else:\n",
    "        w = np.fromfile(f, np.int32, count=1)\n",
    "        h = np.fromfile(f, np.int32, count=1)\n",
    "        print(\"Reading %d x %d flo file\" % (h, w))\n",
    "        data2d = np.fromfile(f, np.float32, count=2 * w * h)\n",
    "        # reshape data into 3D array (columns, rows, channels)\n",
    "        data2d = np.resize(data2d, (h[0], w[0], 2))\n",
    "    f.close()\n",
    "    return data2d\n",
    "\n",
    "\n",
    "# def read_flow_png(flow_file):\n",
    "#     \"\"\"\n",
    "#     Read optical flow from KITTI .png file\n",
    "#     :param flow_file: name of the flow file\n",
    "#     :return: optical flow data in matrix\n",
    "#     \"\"\"\n",
    "#     flow_object = png.Reader(filename=flow_file)\n",
    "#     flow_direct = flow_object.asDirect()\n",
    "#     flow_data = list(flow_direct[2])\n",
    "#     (w, h) = flow_direct[3]['size']\n",
    "#     flow = np.zeros((h, w, 3), dtype=np.float64)\n",
    "#     for i in range(len(flow_data)):\n",
    "#         flow[i, :, 0] = flow_data[i][0::3]\n",
    "#         flow[i, :, 1] = flow_data[i][1::3]\n",
    "#         flow[i, :, 2] = flow_data[i][2::3]\n",
    "\n",
    "#     invalid_idx = (flow[:, :, 2] == 0)\n",
    "#     flow[:, :, 0:2] = (flow[:, :, 0:2] - 2 ** 15) / 64.0\n",
    "#     flow[invalid_idx, 0] = 0\n",
    "#     flow[invalid_idx, 1] = 0\n",
    "#     return flow\n",
    "\n",
    "\n",
    "def write_flow(flow, filename):\n",
    "    \"\"\"\n",
    "    write optical flow in Middlebury .flo format\n",
    "    :param flow: optical flow map\n",
    "    :param filename: optical flow file path to be saved\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    f = open(filename, 'wb')\n",
    "    magic = np.array([202021.25], dtype=np.float32)\n",
    "    (height, width) = flow.shape[0:2]\n",
    "    w = np.array([width], dtype=np.int32)\n",
    "    h = np.array([height], dtype=np.int32)\n",
    "    magic.tofile(f)\n",
    "    w.tofile(f)\n",
    "    h.tofile(f)\n",
    "    flow.tofile(f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def segment_flow(flow):\n",
    "    h = flow.shape[0]\n",
    "    w = flow.shape[1]\n",
    "    u = flow[:, :, 0]\n",
    "    v = flow[:, :, 1]\n",
    "\n",
    "    idx = ((abs(u) > LARGEFLOW) | (abs(v) > LARGEFLOW))\n",
    "    idx2 = (abs(u) == SMALLFLOW)\n",
    "    class0 = (v == 0) & (u == 0)\n",
    "    u[idx2] = 0.00001\n",
    "    tan_value = v / u\n",
    "\n",
    "    class1 = (tan_value < 1) & (tan_value >= 0) & (u > 0) & (v >= 0)\n",
    "    class2 = (tan_value >= 1) & (u >= 0) & (v >= 0)\n",
    "    class3 = (tan_value < -1) & (u <= 0) & (v >= 0)\n",
    "    class4 = (tan_value < 0) & (tan_value >= -1) & (u < 0) & (v >= 0)\n",
    "    class8 = (tan_value >= -1) & (tan_value < 0) & (u > 0) & (v <= 0)\n",
    "    class7 = (tan_value < -1) & (u >= 0) & (v <= 0)\n",
    "    class6 = (tan_value >= 1) & (u <= 0) & (v <= 0)\n",
    "    class5 = (tan_value >= 0) & (tan_value < 1) & (u < 0) & (v <= 0)\n",
    "\n",
    "    seg = np.zeros((h, w))\n",
    "\n",
    "    seg[class1] = 1\n",
    "    seg[class2] = 2\n",
    "    seg[class3] = 3\n",
    "    seg[class4] = 4\n",
    "    seg[class5] = 5\n",
    "    seg[class6] = 6\n",
    "    seg[class7] = 7\n",
    "    seg[class8] = 8\n",
    "    seg[class0] = 0\n",
    "    seg[idx] = 0\n",
    "\n",
    "    return seg\n",
    "\n",
    "\n",
    "def flow_error(tu, tv, u, v):\n",
    "    \"\"\"\n",
    "    Calculate average end point error\n",
    "    :param tu: ground-truth horizontal flow map\n",
    "    :param tv: ground-truth vertical flow map\n",
    "    :param u:  estimated horizontal flow map\n",
    "    :param v:  estimated vertical flow map\n",
    "    :return: End point error of the estimated flow\n",
    "    \"\"\"\n",
    "    smallflow = 0.0\n",
    "    '''\n",
    "    stu = tu[bord+1:end-bord,bord+1:end-bord]\n",
    "    stv = tv[bord+1:end-bord,bord+1:end-bord]\n",
    "    su = u[bord+1:end-bord,bord+1:end-bord]\n",
    "    sv = v[bord+1:end-bord,bord+1:end-bord]\n",
    "    '''\n",
    "    stu = tu[:]\n",
    "    stv = tv[:]\n",
    "    su = u[:]\n",
    "    sv = v[:]\n",
    "\n",
    "    idxUnknow = (abs(stu) > UNKNOWN_FLOW_THRESH) | (abs(stv) > UNKNOWN_FLOW_THRESH)\n",
    "    stu[idxUnknow] = 0\n",
    "    stv[idxUnknow] = 0\n",
    "    su[idxUnknow] = 0\n",
    "    sv[idxUnknow] = 0\n",
    "\n",
    "    ind2 = [(np.absolute(stu) > smallflow) | (np.absolute(stv) > smallflow)]\n",
    "    index_su = su[ind2]\n",
    "    index_sv = sv[ind2]\n",
    "    an = 1.0 / np.sqrt(index_su ** 2 + index_sv ** 2 + 1)\n",
    "    un = index_su * an\n",
    "    vn = index_sv * an\n",
    "\n",
    "    index_stu = stu[ind2]\n",
    "    index_stv = stv[ind2]\n",
    "    tn = 1.0 / np.sqrt(index_stu ** 2 + index_stv ** 2 + 1)\n",
    "    tun = index_stu * tn\n",
    "    tvn = index_stv * tn\n",
    "\n",
    "    '''\n",
    "    angle = un * tun + vn * tvn + (an * tn)\n",
    "    index = [angle == 1.0]\n",
    "    angle[index] = 0.999\n",
    "    ang = np.arccos(angle)\n",
    "    mang = np.mean(ang)\n",
    "    mang = mang * 180 / np.pi\n",
    "    '''\n",
    "\n",
    "    epe = np.sqrt((stu - su) ** 2 + (stv - sv) ** 2)\n",
    "    epe = epe[ind2]\n",
    "    mepe = np.mean(epe)\n",
    "    return mepe\n",
    "\n",
    "\n",
    "def flow_to_image(flow):\n",
    "    \"\"\"\n",
    "    Convert flow into middlebury color code image\n",
    "    :param flow: optical flow map\n",
    "    :return: optical flow image in middlebury color\n",
    "    \"\"\"\n",
    "    u = flow[:, :, 0]\n",
    "    v = flow[:, :, 1]\n",
    "\n",
    "    maxu = -999.\n",
    "    maxv = -999.\n",
    "    minu = 999.\n",
    "    minv = 999.\n",
    "\n",
    "    idxUnknow = (abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH)\n",
    "    u[idxUnknow] = 0\n",
    "    v[idxUnknow] = 0\n",
    "\n",
    "    maxu = max(maxu, np.max(u))\n",
    "    minu = min(minu, np.min(u))\n",
    "\n",
    "    maxv = max(maxv, np.max(v))\n",
    "    minv = min(minv, np.min(v))\n",
    "\n",
    "    rad = np.sqrt(u ** 2 + v ** 2)\n",
    "    maxrad = max(-1, np.max(rad))\n",
    "\n",
    "    print(\"max flow: %.4f\\nflow range:\\nu = %.3f .. %.3f\\nv = %.3f .. %.3f\" % (maxrad, minu,maxu, minv, maxv))\n",
    "\n",
    "    u = u/(maxrad + np.finfo(float).eps)\n",
    "    v = v/(maxrad + np.finfo(float).eps)\n",
    "\n",
    "    img = compute_color(u, v)\n",
    "\n",
    "    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)\n",
    "    img[idx] = 0\n",
    "\n",
    "    return np.uint8(img)\n",
    "\n",
    "\n",
    "def evaluate_flow_file(gt, pred):\n",
    "    \"\"\"\n",
    "    evaluate the estimated optical flow end point error according to ground truth provided\n",
    "    :param gt: ground truth file path\n",
    "    :param pred: estimated optical flow file path\n",
    "    :return: end point error, float32\n",
    "    \"\"\"\n",
    "    # Read flow files and calculate the errors\n",
    "    gt_flow = read_flow(gt)        # ground truth flow\n",
    "    eva_flow = read_flow(pred)     # predicted flow\n",
    "    # Calculate errors\n",
    "    average_pe = flow_error(gt_flow[:, :, 0], gt_flow[:, :, 1], eva_flow[:, :, 0], eva_flow[:, :, 1])\n",
    "    return average_pe\n",
    "\n",
    "\n",
    "def evaluate_flow(gt_flow, pred_flow):\n",
    "    \"\"\"\n",
    "    gt: ground-truth flow\n",
    "    pred: estimated flow\n",
    "    \"\"\"\n",
    "    average_pe = flow_error(gt_flow[:, :, 0], gt_flow[:, :, 1], pred_flow[:, :, 0], pred_flow[:, :, 1])\n",
    "    return average_pe\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "==============\n",
    "Disparity Section\n",
    "==============\n",
    "\"\"\"\n",
    "\n",
    "# \n",
    "# def read_disp_png(file_name):\n",
    "#     \"\"\"\n",
    "#     Read optical flow from KITTI .png file\n",
    "#     :param file_name: name of the flow file\n",
    "#     :return: optical flow data in matrix\n",
    "#     \"\"\"\n",
    "#     image_object = png.Reader(filename=file_name)\n",
    "#     image_direct = image_object.asDirect()\n",
    "#     image_data = list(image_direct[2])\n",
    "#     (w, h) = image_direct[3]['size']\n",
    "#     channel = len(image_data[0]) / w\n",
    "#     flow = np.zeros((h, w, channel), dtype=np.uint16)\n",
    "#     for i in range(len(image_data)):\n",
    "#         for j in range(channel):\n",
    "#             flow[i, :, j] = image_data[i][j::channel]\n",
    "#     return flow[:, :, 0] / 256\n",
    "\n",
    "\n",
    "def disp_to_flowfile(disp, filename):\n",
    "    \"\"\"\n",
    "    Read KITTI disparity file in png format\n",
    "    :param disp: disparity matrix\n",
    "    :param filename: the flow file name to save\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    f = open(filename, 'wb')\n",
    "    magic = np.array([202021.25], dtype=np.float32)\n",
    "    (height, width) = disp.shape[0:2]\n",
    "    w = np.array([width], dtype=np.int32)\n",
    "    h = np.array([height], dtype=np.int32)\n",
    "    empty_map = np.zeros((height, width), dtype=np.float32)\n",
    "    data = np.dstack((disp, empty_map))\n",
    "    magic.tofile(f)\n",
    "    w.tofile(f)\n",
    "    h.tofile(f)\n",
    "    data.tofile(f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "==============\n",
    "Image Section\n",
    "==============\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_image(filename):\n",
    "    \"\"\"\n",
    "    Read normal image of any format\n",
    "    :param filename: name of the image file\n",
    "    :return: image data in matrix uint8 type\n",
    "    \"\"\"\n",
    "    img = Image.open(filename)\n",
    "    im = np.array(img)\n",
    "    return im\n",
    "\n",
    "\n",
    "def warp_image(im, flow):\n",
    "    \"\"\"\n",
    "    Use optical flow to warp image to the next\n",
    "    :param im: image to warp\n",
    "    :param flow: optical flow\n",
    "    :return: warped image\n",
    "    \"\"\"\n",
    "    from scipy import interpolate\n",
    "    image_height = im.shape[0]\n",
    "    image_width = im.shape[1]\n",
    "    flow_height = flow.shape[0]\n",
    "    flow_width = flow.shape[1]\n",
    "    n = image_height * image_width\n",
    "    (iy, ix) = np.mgrid[0:image_height, 0:image_width]\n",
    "    (fy, fx) = np.mgrid[0:flow_height, 0:flow_width]\n",
    "    fx += flow[:,:,0]\n",
    "    fy += flow[:,:,1]\n",
    "    mask = np.logical_or(fx <0 , fx > flow_width)\n",
    "    mask = np.logical_or(mask, fy < 0)\n",
    "    mask = np.logical_or(mask, fy > flow_height)\n",
    "    fx = np.minimum(np.maximum(fx, 0), flow_width)\n",
    "    fy = np.minimum(np.maximum(fy, 0), flow_height)\n",
    "    points = np.concatenate((ix.reshape(n,1), iy.reshape(n,1)), axis=1)\n",
    "    xi = np.concatenate((fx.reshape(n, 1), fy.reshape(n,1)), axis=1)\n",
    "    warp = np.zeros((image_height, image_width, im.shape[2]))\n",
    "    for i in range(im.shape[2]):\n",
    "        channel = im[:, :, i]\n",
    "        plt.imshow(channel, cmap='gray')\n",
    "        values = channel.reshape(n, 1)\n",
    "        new_channel = interpolate.griddata(points, values, xi, method='cubic')\n",
    "        new_channel = np.reshape(new_channel, [flow_height, flow_width])\n",
    "        new_channel[mask] = 1\n",
    "        warp[:, :, i] = new_channel.astype(np.uint8)\n",
    "\n",
    "    return warp.astype(np.uint8)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "==============\n",
    "Others\n",
    "==============\n",
    "\"\"\"\n",
    "\n",
    "def scale_image(image, new_range):\n",
    "    \"\"\"\n",
    "    Linearly scale the image into desired range\n",
    "    :param image: input image\n",
    "    :param new_range: the new range to be aligned\n",
    "    :return: image normalized in new range\n",
    "    \"\"\"\n",
    "    min_val = np.min(image).astype(np.float32)\n",
    "    max_val = np.max(image).astype(np.float32)\n",
    "    min_val_new = np.array(min(new_range), dtype=np.float32)\n",
    "    max_val_new = np.array(max(new_range), dtype=np.float32)\n",
    "    scaled_image = (image - min_val) / (max_val - min_val) * (max_val_new - min_val_new) + min_val_new\n",
    "    return scaled_image.astype(np.uint8)\n",
    "\n",
    "\n",
    "def compute_color(u, v):\n",
    "    \"\"\"\n",
    "    compute optical flow color map\n",
    "    :param u: optical flow horizontal map\n",
    "    :param v: optical flow vertical map\n",
    "    :return: optical flow in color code\n",
    "    \"\"\"\n",
    "    [h, w] = u.shape\n",
    "    img = np.zeros([h, w, 3])\n",
    "    nanIdx = np.isnan(u) | np.isnan(v)\n",
    "    u[nanIdx] = 0\n",
    "    v[nanIdx] = 0\n",
    "\n",
    "    colorwheel = make_color_wheel()\n",
    "    ncols = np.size(colorwheel, 0)\n",
    "\n",
    "    rad = np.sqrt(u**2+v**2)\n",
    "\n",
    "    a = np.arctan2(-v, -u) / np.pi\n",
    "\n",
    "    fk = (a+1) / 2 * (ncols - 1) + 1\n",
    "\n",
    "    k0 = np.floor(fk).astype(int)\n",
    "\n",
    "    k1 = k0 + 1\n",
    "    k1[k1 == ncols+1] = 1\n",
    "    f = fk - k0\n",
    "\n",
    "    for i in range(0, np.size(colorwheel,1)):\n",
    "        tmp = colorwheel[:, i]\n",
    "        col0 = tmp[k0-1] / 255\n",
    "        col1 = tmp[k1-1] / 255\n",
    "        col = (1-f) * col0 + f * col1\n",
    "\n",
    "        idx = rad <= 1\n",
    "        col[idx] = 1-rad[idx]*(1-col[idx])\n",
    "        notidx = np.logical_not(idx)\n",
    "\n",
    "        col[notidx] *= 0.75\n",
    "        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_color_wheel():\n",
    "    \"\"\"\n",
    "    Generate color wheel according Middlebury color code\n",
    "    :return: Color wheel\n",
    "    \"\"\"\n",
    "    RY = 15\n",
    "    YG = 6\n",
    "    GC = 4\n",
    "    CB = 11\n",
    "    BM = 13\n",
    "    MR = 6\n",
    "\n",
    "    ncols = RY + YG + GC + CB + BM + MR\n",
    "\n",
    "    colorwheel = np.zeros([ncols, 3])\n",
    "\n",
    "    col = 0\n",
    "\n",
    "    # RY\n",
    "    colorwheel[0:RY, 0] = 255\n",
    "    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n",
    "    col += RY\n",
    "\n",
    "    # YG\n",
    "    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n",
    "    colorwheel[col:col+YG, 1] = 255\n",
    "    col += YG\n",
    "\n",
    "    # GC\n",
    "    colorwheel[col:col+GC, 1] = 255\n",
    "    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n",
    "    col += GC\n",
    "\n",
    "    # CB\n",
    "    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n",
    "    colorwheel[col:col+CB, 2] = 255\n",
    "    col += CB\n",
    "\n",
    "    # BM\n",
    "    colorwheel[col:col+BM, 2] = 255\n",
    "    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n",
    "    col += + BM\n",
    "\n",
    "    # MR\n",
    "    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
    "    colorwheel[col:col+MR, 0] = 255\n",
    "\n",
    "    return colorwheel\n",
    "\n",
    "def plot_feature_dim(lstm_input):\n",
    "    fig, axs = plt.subplots(4,3,figsize=(15,15))\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            if (i == 3 and j == 2):\n",
    "                break\n",
    "\n",
    "            x = np.arange(0,80000)\n",
    "            axs[i,j].plot(x,lstm_input[:,i*3+j])\n",
    "            axs[i,j].set_title(f'Feature {i*3+j+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the source video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('data/caolin_foot_trial1-4.mp4')\n",
    "name = 'caolin'\n",
    "\n",
    "framerate = cap.get(5)\n",
    "total_image = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f'The oringinal framerate is {cap.get(5)} with frame resolution of: {cap.get(3), cap.get(4)}')\n",
    "print(f'The total number of frame in this video is {total_image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "while(cap.isOpened()):\n",
    "    frameID = cap.get(1) # get the current frame number\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if(ret != True):\n",
    "        print(f'We\\'ve gotten {int(frameID/5)+1} frames from this video.')\n",
    "        break\n",
    "    # Take at 5Hz frequency which is framerate/5\n",
    "    if (frameID % int(framerate/5) == 0):\n",
    "        frame = frame[64:, 162:610 , :]\n",
    "        filename = 'Train_1/'+ name + \"_frame%04d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Path: /home/projectx/Documents/other people's repo/flownet2-pytorch\n"
     ]
    }
   ],
   "source": [
    "print(\"Directory Path:\", Path().absolute())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    \n",
    "    def __init__(self, num_train_img = 3400, base_dir = Path().absolute(), sequence = Path('Train_1/')):  # base_dir(image) and sequence(lstm) are directories\n",
    "        self.base_dir = base_dir\n",
    "        self.sequence = sequence\n",
    "        self.base_path_img = os.path.join(self.base_dir, self.sequence)\n",
    "        \n",
    "#         self.base_dir + self.sequence\n",
    "#         os.path.join(\"source_data\", \"text_files\")\n",
    "        \n",
    "        self.image_files = os.listdir(self.base_path_img)\n",
    "        self.image_files.sort()\n",
    "        self.image_files = self.image_files[0:num_train_img]\n",
    "        \n",
    "        # normalization for lstm data\n",
    "        self.train_scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "        ## Omega.7 and load cells\n",
    "        self.input_lstm = self.read_OMEGA7_LC()\n",
    "        \n",
    "        \n",
    "        self.imu_seq_len = 20\n",
    "    \n",
    "    def read_OMEGA7_LC(self, path='data/result_1.csv'):\n",
    "        # read csv data\n",
    "        df = pd.read_csv(path,header = None)\n",
    "        df = df[:874300]\n",
    "        \n",
    "        # take moving average of every 10 data points\n",
    "        new_df = df.groupby(df.index//10).mean()\n",
    "        array_input = np.array(new_df)\n",
    "        \n",
    "        # normalization\n",
    "        array_input_scaled = self.train_scaler.fit_transform(array_input)\n",
    "        \n",
    "        input_lstm  = Variable(torch.from_numpy(array_input_scaled).type(torch.FloatTensor))\n",
    "        input_lstm = input_lstm[:80000,:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # reshape to (num_dataset, sequence_length, feature_size)\n",
    "        input_lstm = input_lstm.view(-1,20,11)\n",
    "        \n",
    "        return np.array(input_lstm)\n",
    "    \n",
    "    \n",
    "    def get_input_lstm(self):\n",
    "        return self.input_lstm\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def load_img_bat(self, idx, batch):\n",
    "        batch_x = []\n",
    "        batch_input_lstm = []\n",
    "        for i in range(batch):\n",
    "            x_data_np_1 = np.array(Image.open(os.path.join(self.base_path_img, Path(self.image_files[idx + i]))))\n",
    "            x_data_np_2 = np.array(Image.open(os.path.join(self.base_path_img, Path(self.image_files[idx+1 + i]))))\n",
    "            x_data_np_1 = x_data_np_1.reshape(3,512,448)\n",
    "            x_data_np_2 = x_data_np_2.reshape(3,512,448)\n",
    "\n",
    "#             ## 3 channels\n",
    "#             x_data_np_1 = np.array([x_data_np_1, x_data_np_1, x_data_np_1])\n",
    "#             x_data_np_2 = np.array([x_data_np_2, x_data_np_2, x_data_np_2])\n",
    "\n",
    "            X = np.array([x_data_np_1, x_data_np_2])\n",
    "            batch_x.append(X)\n",
    "        \n",
    "#           self.input_lstm of size: (num_dataset, sequence_length, feature_size)\n",
    "            tmp = np.array(self.input_lstm[idx + i])\n",
    "            batch_input_lstm.append(tmp)\n",
    "            \n",
    "        \n",
    "        batch_x = np.array(batch_x).transpose( 0, 2, 1, 3,4)\n",
    "        batch_input_lstm = np.array(batch_input_lstm)\n",
    "        \n",
    "        X = Variable(torch.from_numpy(batch_x).type(torch.FloatTensor).cuda())    \n",
    "        X2 = Variable(torch.from_numpy(batch_input_lstm).type(torch.FloatTensor).cuda())    \n",
    "        \n",
    "        Y = X2[:,:,-3:]\n",
    "        \n",
    "     \n",
    "        return X, X2 , Y.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MyDataset()\n",
    "X, X2 , Y = dataset.load_img_bat(0,1)\n",
    "X.size(), X2.size() , Y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data = dataset.get_input_lstm()\n",
    "# Though there are 4000, we will only call first 3400, constrained by the length of 'num_train_img'\n",
    "lstm_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = dataset.read_OMEGA7_LC()\n",
    "lstm_input = lstm_input.reshape(4000*20, -1)\n",
    "plot_feature_dim(lstm_input)\n",
    "lstm_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "class Surgical_VINet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Surgical_VINet, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=28792 ,#49152,#24576, \n",
    "            hidden_size=256,#64, \n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnn.cuda()\n",
    "        \n",
    "        self.rnnIMU = nn.LSTM(\n",
    "            input_size=11, \n",
    "            hidden_size=6,\n",
    "            num_layers=2,\n",
    "            batch_first=True)\n",
    "        self.rnnIMU.cuda()\n",
    "        \n",
    "        self.linear1 = nn.Linear(256, 128)\n",
    "        self.linear2 = nn.Linear(128, 3)\n",
    "        #self.linear3 = nn.Linear(128, 6)\n",
    "        self.linear1.cuda()\n",
    "        self.linear2.cuda()\n",
    "        #self.linear3.cuda()\n",
    "        \n",
    "        # load checkpoint state from NVIDIA training\n",
    "           \n",
    "        print('....creating model....')\n",
    "        self.flownet_c = FlowNet2(args).cuda()\n",
    "        \n",
    "        print('....loading weights....')\n",
    "        self.state_dict = torch.load(\"FlowNet2_checkpoint.pth.tar\")\n",
    "        self.flownet_c.load_state_dict(self.state_dict[\"state_dict\"])\n",
    "        self.flownet_c.cuda()\n",
    "        \n",
    "        print('....model created....')\n",
    "        for param in self.flownet_c.parameters():\n",
    "            param.requires_grad = False\n",
    "        print('....model weights freezed....')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, image, imu):\n",
    "        \n",
    "        c_out = self.flownet_c(image) #[2, 128, 112] # [1, 2, 8, 7]\n",
    "#         print('c_out', c_out.shape)\n",
    "        \n",
    "        ## Input2: Feed IMU records to LSTM\n",
    "        imu_out, (imu_n, imu_c) = self.rnnIMU(imu)\n",
    "        # to match Vision output shape\n",
    "        imu_out = imu_out.view(batch_size,1,-1)   # (batch_size, 1, total_hidden_size) [1, 1, 120]\n",
    "        \n",
    "#         print('imu_out', imu_out.shape)\n",
    "        \n",
    "        \n",
    "        ## Combine the output of input1 and 2 and feed it to LSTM\n",
    "        #r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_in = c_out.view(batch_size, 1, -1) # [1,1,28672] # [1,1,112]\n",
    "#         print('r_in', r_in.shape)\n",
    "        \n",
    "\n",
    "        cat_out = torch.cat((r_in, imu_out), 2)#[1 1 28792]\n",
    "#         print(cat_out.shape)\n",
    "        \n",
    "        \n",
    "        r_out, (h_n, h_c) = self.rnn(cat_out)  # (1, 1, 256)\n",
    "#         print('r_out', r_out.shape)\n",
    "        l_out1 = self.linear1(r_out[:,-1,:])\n",
    "        l_out2 = self.linear2(l_out1)\n",
    "        \n",
    "#         print('r_ol_out2ut', l_out2.shape)\n",
    "        #l_out3 = self.linear3(l_out2)\n",
    "\n",
    "        return l_out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # hyper-parameters\n",
    "    epoch = 2\n",
    "    batch = 1\n",
    "    lr = 0.001\n",
    "    \n",
    "    # creat model and dataset\n",
    "    model = Surgical_VINet()\n",
    "    mydataset = MyDataset()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#     optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # TensorBoard\n",
    "    writer = SummaryWriter()\n",
    " \n",
    "    model.train()\n",
    "    \n",
    "    mydataset = MyDataset()\n",
    "    \n",
    "#     criterion = nn.MSELoss()\n",
    "    criterion  = nn.L1Loss(size_average=False)\n",
    "    \n",
    "    start = 5\n",
    "    end = len(mydataset) - batch\n",
    "    batch_num = (end - start)\n",
    "    startT = time.time()\n",
    "    \n",
    "    with tools.TimerBlock(\"Start training\") as block :\n",
    "        for k in range(epoch):\n",
    "            for i in range(start, end):\n",
    "                data, data_lstm, target = mydataset.load_img_bat(i, batch)\n",
    "#                 data, data_lstm, target = data.cuda(), data_lstm.cuda(), target.cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward pass\n",
    "                output = model(data, data_lstm)\n",
    "                \n",
    "                # compute loss\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_value_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                avgTime = block.avg()\n",
    "                remainingTime = int((batch_num*epoch -  (i + batch_num*k)) * avgTime)\n",
    "                rTime_str = \"{:02d}:{:02d}:{:02d}\".format(int(remainingTime/60//60), \n",
    "                                                          int(remainingTime//60%60), \n",
    "                                                          int(remainingTime%60))\n",
    "                block.log(f'Train Epoch: {k}\\t[{i}/{batch_num} ({(100.*(i + batch_num*k)):.0f}%)]\\tLoss: {loss.data:.6f}, TimeAvg: {avgTime:.4f}, Remaining: {rTime_str}')\n",
    "                          \n",
    "                writer.add_scalar('loss', loss.data, k*batch_num + i)\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "            check_str = 'checkpoint_{}.pt'.format(k)\n",
    "            if (k+1)%5 == 0:\n",
    "                torch.save(model.state_dict(), check_str)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'test_network_trial_1.pt')\n",
    "    writer.export_scalars_to_json(\"./summary_writer.json\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....creating model....\n",
      "....loading weights....\n",
      "....model created....\n",
      "....model weights freezed....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:1: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/projectx/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "  [0.477s] Train Epoch: 0\t[5/3394 (500%)]\tLoss: 2.145501, TimeAvg: 0.4490, Remaining: 00:50:45\n",
      "  [0.838s] Train Epoch: 0\t[6/3394 (600%)]\tLoss: 1.621291, TimeAvg: 0.8294, Remaining: 01:33:45\n",
      "  [1.210s] Train Epoch: 0\t[7/3394 (700%)]\tLoss: 1.328026, TimeAvg: 0.6010, Remaining: 01:07:55\n",
      "  [1.579s] Train Epoch: 0\t[8/3394 (800%)]\tLoss: 1.126436, TimeAvg: 0.5232, Remaining: 00:59:07\n",
      "  [1.942s] Train Epoch: 0\t[9/3394 (900%)]\tLoss: 1.004346, TimeAvg: 0.4834, Remaining: 00:54:37\n",
      "  [2.286s] Train Epoch: 0\t[10/3394 (1000%)]\tLoss: 0.761315, TimeAvg: 0.4554, Remaining: 00:51:26\n",
      "  [2.631s] Train Epoch: 0\t[11/3394 (1100%)]\tLoss: 0.398882, TimeAvg: 0.4361, Remaining: 00:49:15\n",
      "  [3.123s] Train Epoch: 0\t[12/3394 (1200%)]\tLoss: 0.125067, TimeAvg: 0.4441, Remaining: 00:50:09\n",
      "  [3.462s] Train Epoch: 0\t[13/3394 (1300%)]\tLoss: 0.472103, TimeAvg: 0.4311, Remaining: 00:48:40\n",
      "  [3.796s] Train Epoch: 0\t[14/3394 (1400%)]\tLoss: 0.586899, TimeAvg: 0.4206, Remaining: 00:47:29\n",
      "  [4.131s] Train Epoch: 0\t[15/3394 (1500%)]\tLoss: 0.686035, TimeAvg: 0.4121, Remaining: 00:46:31\n",
      "  [4.460s] Train Epoch: 0\t[16/3394 (1600%)]\tLoss: 0.879097, TimeAvg: 0.4048, Remaining: 00:45:41\n",
      "  [4.795s] Train Epoch: 0\t[17/3394 (1700%)]\tLoss: 2.177239, TimeAvg: 0.3987, Remaining: 00:44:59\n",
      "  [5.130s] Train Epoch: 0\t[18/3394 (1800%)]\tLoss: 1.577708, TimeAvg: 0.3939, Remaining: 00:44:26\n",
      "  [5.505s] Train Epoch: 0\t[19/3394 (1900%)]\tLoss: 1.883024, TimeAvg: 0.3925, Remaining: 00:44:16\n",
      "  [5.835s] Train Epoch: 0\t[20/3394 (2000%)]\tLoss: 0.864906, TimeAvg: 0.3883, Remaining: 00:43:48\n",
      "  [6.175s] Train Epoch: 0\t[21/3394 (2100%)]\tLoss: 1.575701, TimeAvg: 0.3853, Remaining: 00:43:27\n",
      "  [6.507s] Train Epoch: 0\t[22/3394 (2200%)]\tLoss: 0.633603, TimeAvg: 0.3821, Remaining: 00:43:05\n",
      "  [6.837s] Train Epoch: 0\t[23/3394 (2300%)]\tLoss: 0.543354, TimeAvg: 0.3792, Remaining: 00:42:45\n",
      "  [7.234s] Train Epoch: 0\t[24/3394 (2400%)]\tLoss: 1.598305, TimeAvg: 0.3802, Remaining: 00:42:51\n",
      "  [7.573s] Train Epoch: 0\t[25/3394 (2500%)]\tLoss: 1.332655, TimeAvg: 0.3781, Remaining: 00:42:37\n",
      "  [7.905s] Train Epoch: 0\t[26/3394 (2600%)]\tLoss: 0.596167, TimeAvg: 0.3759, Remaining: 00:42:22\n",
      "  [8.252s] Train Epoch: 0\t[27/3394 (2700%)]\tLoss: 0.834916, TimeAvg: 0.3744, Remaining: 00:42:11\n",
      "  [8.649s] Train Epoch: 0\t[28/3394 (2800%)]\tLoss: 0.519802, TimeAvg: 0.3758, Remaining: 00:42:20\n",
      "  [9.035s] Train Epoch: 0\t[29/3394 (2900%)]\tLoss: 1.333596, TimeAvg: 0.3756, Remaining: 00:42:18\n",
      "  [9.414s] Train Epoch: 0\t[30/3394 (3000%)]\tLoss: 0.943359, TimeAvg: 0.3761, Remaining: 00:42:21\n",
      "  [9.745s] Train Epoch: 0\t[31/3394 (3100%)]\tLoss: 0.780605, TimeAvg: 0.3744, Remaining: 00:42:09\n",
      "  [10.076s] Train Epoch: 0\t[32/3394 (3200%)]\tLoss: 0.452975, TimeAvg: 0.3728, Remaining: 00:41:58\n",
      "  [10.414s] Train Epoch: 0\t[33/3394 (3300%)]\tLoss: 0.365948, TimeAvg: 0.3716, Remaining: 00:41:49\n",
      "  [10.744s] Train Epoch: 0\t[34/3394 (3400%)]\tLoss: 0.214498, TimeAvg: 0.3700, Remaining: 00:41:38\n",
      "  [11.095s] Train Epoch: 0\t[35/3394 (3500%)]\tLoss: 0.679668, TimeAvg: 0.3693, Remaining: 00:41:34\n",
      "  [11.456s] Train Epoch: 0\t[36/3394 (3600%)]\tLoss: 0.697112, TimeAvg: 0.3693, Remaining: 00:41:33\n",
      "  [11.842s] Train Epoch: 0\t[37/3394 (3700%)]\tLoss: 0.561604, TimeAvg: 0.3699, Remaining: 00:41:37\n",
      "  [12.198s] Train Epoch: 0\t[38/3394 (3800%)]\tLoss: 0.174227, TimeAvg: 0.3693, Remaining: 00:41:33\n",
      "  [12.535s] Train Epoch: 0\t[39/3394 (3900%)]\tLoss: 0.424691, TimeAvg: 0.3684, Remaining: 00:41:26\n",
      "  [12.867s] Train Epoch: 0\t[40/3394 (4000%)]\tLoss: 0.796604, TimeAvg: 0.3674, Remaining: 00:41:18\n",
      "  [13.201s] Train Epoch: 0\t[41/3394 (4100%)]\tLoss: 0.792424, TimeAvg: 0.3665, Remaining: 00:41:12\n",
      "  [13.550s] Train Epoch: 0\t[42/3394 (4200%)]\tLoss: 0.787497, TimeAvg: 0.3660, Remaining: 00:41:08\n",
      "  [13.896s] Train Epoch: 0\t[43/3394 (4300%)]\tLoss: 0.733257, TimeAvg: 0.3654, Remaining: 00:41:04\n",
      "  [14.227s] Train Epoch: 0\t[44/3394 (4400%)]\tLoss: 0.672288, TimeAvg: 0.3645, Remaining: 00:40:58\n",
      "  [14.568s] Train Epoch: 0\t[45/3394 (4500%)]\tLoss: 0.698037, TimeAvg: 0.3639, Remaining: 00:40:53\n",
      "  [14.899s] Train Epoch: 0\t[46/3394 (4600%)]\tLoss: 0.440689, TimeAvg: 0.3631, Remaining: 00:40:47\n",
      "  [15.231s] Train Epoch: 0\t[47/3394 (4700%)]\tLoss: 0.207167, TimeAvg: 0.3623, Remaining: 00:40:42\n",
      "  [15.569s] Train Epoch: 0\t[48/3394 (4800%)]\tLoss: 0.273674, TimeAvg: 0.3618, Remaining: 00:40:38\n",
      "  [15.898s] Train Epoch: 0\t[49/3394 (4900%)]\tLoss: 0.560066, TimeAvg: 0.3610, Remaining: 00:40:32\n",
      "  [16.228s] Train Epoch: 0\t[50/3394 (5000%)]\tLoss: 0.570115, TimeAvg: 0.3603, Remaining: 00:40:27\n",
      "  [16.564s] Train Epoch: 0\t[51/3394 (5100%)]\tLoss: 0.478051, TimeAvg: 0.3598, Remaining: 00:40:23\n",
      "  [16.893s] Train Epoch: 0\t[52/3394 (5200%)]\tLoss: 0.309452, TimeAvg: 0.3591, Remaining: 00:40:19\n",
      "  [17.221s] Train Epoch: 0\t[53/3394 (5300%)]\tLoss: 0.553553, TimeAvg: 0.3585, Remaining: 00:40:14\n",
      "  [17.561s] Train Epoch: 0\t[54/3394 (5400%)]\tLoss: 0.736237, TimeAvg: 0.3584, Remaining: 00:40:13\n",
      "  [17.908s] Train Epoch: 0\t[55/3394 (5500%)]\tLoss: 0.876911, TimeAvg: 0.3579, Remaining: 00:40:09\n",
      "  [18.237s] Train Epoch: 0\t[56/3394 (5600%)]\tLoss: 0.994782, TimeAvg: 0.3573, Remaining: 00:40:05\n",
      "  [18.583s] Train Epoch: 0\t[57/3394 (5700%)]\tLoss: 1.279545, TimeAvg: 0.3572, Remaining: 00:40:04\n",
      "  [18.915s] Train Epoch: 0\t[58/3394 (5800%)]\tLoss: 1.388347, TimeAvg: 0.3567, Remaining: 00:40:00\n",
      "  [19.259s] Train Epoch: 0\t[59/3394 (5900%)]\tLoss: 1.333240, TimeAvg: 0.3565, Remaining: 00:39:58\n",
      "  [19.615s] Train Epoch: 0\t[60/3394 (6000%)]\tLoss: 1.309408, TimeAvg: 0.3565, Remaining: 00:39:58\n",
      "  [19.966s] Train Epoch: 0\t[61/3394 (6100%)]\tLoss: 1.341260, TimeAvg: 0.3563, Remaining: 00:39:57\n",
      "  [20.342s] Train Epoch: 0\t[62/3394 (6200%)]\tLoss: 1.387539, TimeAvg: 0.3568, Remaining: 00:39:59\n",
      "  [20.717s] Train Epoch: 0\t[63/3394 (6300%)]\tLoss: 1.853714, TimeAvg: 0.3569, Remaining: 00:40:00\n",
      "  [21.065s] Train Epoch: 0\t[64/3394 (6400%)]\tLoss: 2.017390, TimeAvg: 0.3568, Remaining: 00:39:59\n",
      "  [21.394s] Train Epoch: 0\t[65/3394 (6500%)]\tLoss: 2.058257, TimeAvg: 0.3563, Remaining: 00:39:55\n",
      "  [21.729s] Train Epoch: 0\t[66/3394 (6600%)]\tLoss: 2.438852, TimeAvg: 0.3560, Remaining: 00:39:52\n",
      "  [22.059s] Train Epoch: 0\t[67/3394 (6700%)]\tLoss: 2.522548, TimeAvg: 0.3556, Remaining: 00:39:49\n",
      "  [22.388s] Train Epoch: 0\t[68/3394 (6800%)]\tLoss: 2.272488, TimeAvg: 0.3552, Remaining: 00:39:46\n",
      "  [22.733s] Train Epoch: 0\t[69/3394 (6900%)]\tLoss: 2.112631, TimeAvg: 0.3551, Remaining: 00:39:45\n",
      "  [23.118s] Train Epoch: 0\t[70/3394 (7000%)]\tLoss: 1.821865, TimeAvg: 0.3555, Remaining: 00:39:48\n",
      "  [23.502s] Train Epoch: 0\t[71/3394 (7100%)]\tLoss: 1.779746, TimeAvg: 0.3559, Remaining: 00:39:50\n",
      "  [23.865s] Train Epoch: 0\t[72/3394 (7200%)]\tLoss: 0.984619, TimeAvg: 0.3561, Remaining: 00:39:51\n",
      "  [24.211s] Train Epoch: 0\t[73/3394 (7300%)]\tLoss: 0.180270, TimeAvg: 0.3559, Remaining: 00:39:49\n",
      "  [24.541s] Train Epoch: 0\t[74/3394 (7400%)]\tLoss: 0.507546, TimeAvg: 0.3555, Remaining: 00:39:46\n",
      "  [24.890s] Train Epoch: 0\t[75/3394 (7500%)]\tLoss: 0.955715, TimeAvg: 0.3554, Remaining: 00:39:46\n",
      "  [25.259s] Train Epoch: 0\t[76/3394 (7600%)]\tLoss: 1.660160, TimeAvg: 0.3556, Remaining: 00:39:47\n",
      "  [25.628s] Train Epoch: 0\t[77/3394 (7700%)]\tLoss: 2.372921, TimeAvg: 0.3558, Remaining: 00:39:47\n",
      "  [25.988s] Train Epoch: 0\t[78/3394 (7800%)]\tLoss: 2.630276, TimeAvg: 0.3559, Remaining: 00:39:48\n",
      "  [26.331s] Train Epoch: 0\t[79/3394 (7900%)]\tLoss: 2.577123, TimeAvg: 0.3557, Remaining: 00:39:46\n",
      "  [26.667s] Train Epoch: 0\t[80/3394 (8000%)]\tLoss: 2.575590, TimeAvg: 0.3554, Remaining: 00:39:43\n",
      "  [27.029s] Train Epoch: 0\t[81/3394 (8100%)]\tLoss: 2.613758, TimeAvg: 0.3555, Remaining: 00:39:44\n",
      "  [27.397s] Train Epoch: 0\t[82/3394 (8200%)]\tLoss: 2.627366, TimeAvg: 0.3557, Remaining: 00:39:45\n",
      "  [27.749s] Train Epoch: 0\t[83/3394 (8300%)]\tLoss: 2.520984, TimeAvg: 0.3556, Remaining: 00:39:44\n",
      "  [28.091s] Train Epoch: 0\t[84/3394 (8400%)]\tLoss: 2.365602, TimeAvg: 0.3554, Remaining: 00:39:42\n",
      "  [28.423s] Train Epoch: 0\t[85/3394 (8500%)]\tLoss: 2.213202, TimeAvg: 0.3552, Remaining: 00:39:40\n",
      "  [28.754s] Train Epoch: 0\t[86/3394 (8600%)]\tLoss: 2.155293, TimeAvg: 0.3549, Remaining: 00:39:38\n",
      "  [29.095s] Train Epoch: 0\t[87/3394 (8700%)]\tLoss: 2.088890, TimeAvg: 0.3547, Remaining: 00:39:36\n",
      "  [29.426s] Train Epoch: 0\t[88/3394 (8800%)]\tLoss: 2.031331, TimeAvg: 0.3544, Remaining: 00:39:34\n",
      "  [29.758s] Train Epoch: 0\t[89/3394 (8900%)]\tLoss: 1.957072, TimeAvg: 0.3541, Remaining: 00:39:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [30.113s] Train Epoch: 0\t[90/3394 (9000%)]\tLoss: 1.882463, TimeAvg: 0.3541, Remaining: 00:39:31\n",
      "  [30.530s] Train Epoch: 0\t[91/3394 (9100%)]\tLoss: 1.757292, TimeAvg: 0.3549, Remaining: 00:39:36\n",
      "  [30.901s] Train Epoch: 0\t[92/3394 (9200%)]\tLoss: 1.647303, TimeAvg: 0.3550, Remaining: 00:39:37\n",
      "  [31.240s] Train Epoch: 0\t[93/3394 (9300%)]\tLoss: 1.498556, TimeAvg: 0.3549, Remaining: 00:39:35\n",
      "  [31.572s] Train Epoch: 0\t[94/3394 (9400%)]\tLoss: 1.311124, TimeAvg: 0.3546, Remaining: 00:39:33\n",
      "  [31.910s] Train Epoch: 0\t[95/3394 (9500%)]\tLoss: 2.440348, TimeAvg: 0.3544, Remaining: 00:39:32\n",
      "  [32.255s] Train Epoch: 0\t[96/3394 (9600%)]\tLoss: 0.982241, TimeAvg: 0.3544, Remaining: 00:39:31\n",
      "  [32.634s] Train Epoch: 0\t[97/3394 (9700%)]\tLoss: 1.433155, TimeAvg: 0.3546, Remaining: 00:39:32\n",
      "  [32.993s] Train Epoch: 0\t[98/3394 (9800%)]\tLoss: 0.213707, TimeAvg: 0.3547, Remaining: 00:39:32\n",
      "  [33.327s] Train Epoch: 0\t[99/3394 (9900%)]\tLoss: 0.237679, TimeAvg: 0.3544, Remaining: 00:39:30\n",
      "  [33.660s] Train Epoch: 0\t[100/3394 (10000%)]\tLoss: 0.472559, TimeAvg: 0.3542, Remaining: 00:39:28\n",
      "  [33.999s] Train Epoch: 0\t[101/3394 (10100%)]\tLoss: 0.588355, TimeAvg: 0.3541, Remaining: 00:39:27\n",
      "  [34.348s] Train Epoch: 0\t[102/3394 (10200%)]\tLoss: 0.385900, TimeAvg: 0.3540, Remaining: 00:39:26\n",
      "  [34.739s] Train Epoch: 0\t[103/3394 (10300%)]\tLoss: 0.431806, TimeAvg: 0.3544, Remaining: 00:39:29\n",
      "  [35.126s] Train Epoch: 0\t[104/3394 (10400%)]\tLoss: 0.415443, TimeAvg: 0.3548, Remaining: 00:39:31\n",
      "  [35.476s] Train Epoch: 0\t[105/3394 (10500%)]\tLoss: 0.330393, TimeAvg: 0.3546, Remaining: 00:39:30\n",
      "  [35.806s] Train Epoch: 0\t[106/3394 (10600%)]\tLoss: 0.882107, TimeAvg: 0.3544, Remaining: 00:39:27\n",
      "  [36.143s] Train Epoch: 0\t[107/3394 (10700%)]\tLoss: 1.715536, TimeAvg: 0.3542, Remaining: 00:39:26\n",
      "  [36.476s] Train Epoch: 0\t[108/3394 (10800%)]\tLoss: 2.048613, TimeAvg: 0.3540, Remaining: 00:39:24\n",
      "  [36.807s] Train Epoch: 0\t[109/3394 (10900%)]\tLoss: 1.949103, TimeAvg: 0.3538, Remaining: 00:39:22\n",
      "  [37.145s] Train Epoch: 0\t[110/3394 (11000%)]\tLoss: 1.666544, TimeAvg: 0.3536, Remaining: 00:39:21\n",
      "  [37.477s] Train Epoch: 0\t[111/3394 (11100%)]\tLoss: 1.491984, TimeAvg: 0.3534, Remaining: 00:39:19\n",
      "  [37.808s] Train Epoch: 0\t[112/3394 (11200%)]\tLoss: 1.290830, TimeAvg: 0.3532, Remaining: 00:39:18\n",
      "  [38.166s] Train Epoch: 0\t[113/3394 (11300%)]\tLoss: 1.092386, TimeAvg: 0.3533, Remaining: 00:39:18\n",
      "  [38.552s] Train Epoch: 0\t[114/3394 (11400%)]\tLoss: 1.045491, TimeAvg: 0.3536, Remaining: 00:39:19\n",
      "  [38.932s] Train Epoch: 0\t[115/3394 (11500%)]\tLoss: 0.954380, TimeAvg: 0.3539, Remaining: 00:39:21\n",
      "  [39.280s] Train Epoch: 0\t[116/3394 (11600%)]\tLoss: 0.868857, TimeAvg: 0.3537, Remaining: 00:39:20\n",
      "  [39.612s] Train Epoch: 0\t[117/3394 (11700%)]\tLoss: 0.766861, TimeAvg: 0.3536, Remaining: 00:39:18\n",
      "  [39.945s] Train Epoch: 0\t[118/3394 (11800%)]\tLoss: 0.638040, TimeAvg: 0.3534, Remaining: 00:39:17\n",
      "  [40.315s] Train Epoch: 0\t[119/3394 (11900%)]\tLoss: 0.521809, TimeAvg: 0.3535, Remaining: 00:39:17\n",
      "  [40.690s] Train Epoch: 0\t[120/3394 (12000%)]\tLoss: 0.454395, TimeAvg: 0.3536, Remaining: 00:39:18\n",
      "  [41.053s] Train Epoch: 0\t[121/3394 (12100%)]\tLoss: 0.416878, TimeAvg: 0.3538, Remaining: 00:39:19\n",
      "  [41.413s] Train Epoch: 0\t[122/3394 (12200%)]\tLoss: 0.376580, TimeAvg: 0.3538, Remaining: 00:39:18\n",
      "  [41.746s] Train Epoch: 0\t[123/3394 (12300%)]\tLoss: 0.378383, TimeAvg: 0.3537, Remaining: 00:39:17\n",
      "  [42.086s] Train Epoch: 0\t[124/3394 (12400%)]\tLoss: 0.464599, TimeAvg: 0.3536, Remaining: 00:39:16\n",
      "  [42.466s] Train Epoch: 0\t[125/3394 (12500%)]\tLoss: 0.578403, TimeAvg: 0.3538, Remaining: 00:39:17\n",
      "  [42.856s] Train Epoch: 0\t[126/3394 (12600%)]\tLoss: 0.684770, TimeAvg: 0.3541, Remaining: 00:39:18\n",
      "  [43.243s] Train Epoch: 0\t[127/3394 (12700%)]\tLoss: 0.784717, TimeAvg: 0.3544, Remaining: 00:39:20\n",
      "  [43.598s] Train Epoch: 0\t[128/3394 (12800%)]\tLoss: 0.762388, TimeAvg: 0.3543, Remaining: 00:39:19\n",
      "  [43.942s] Train Epoch: 0\t[129/3394 (12900%)]\tLoss: 0.729731, TimeAvg: 0.3543, Remaining: 00:39:19\n",
      "  [44.327s] Train Epoch: 0\t[130/3394 (13000%)]\tLoss: 0.540157, TimeAvg: 0.3546, Remaining: 00:39:20\n",
      "  [44.721s] Train Epoch: 0\t[131/3394 (13100%)]\tLoss: 0.395169, TimeAvg: 0.3549, Remaining: 00:39:22\n",
      "  [45.068s] Train Epoch: 0\t[132/3394 (13200%)]\tLoss: 0.395710, TimeAvg: 0.3548, Remaining: 00:39:21\n",
      "  [45.400s] Train Epoch: 0\t[133/3394 (13300%)]\tLoss: 0.185693, TimeAvg: 0.3546, Remaining: 00:39:19\n",
      "  [45.762s] Train Epoch: 0\t[134/3394 (13400%)]\tLoss: 0.184670, TimeAvg: 0.3547, Remaining: 00:39:20\n",
      "  [46.150s] Train Epoch: 0\t[135/3394 (13500%)]\tLoss: 0.416582, TimeAvg: 0.3549, Remaining: 00:39:21\n",
      "  [46.533s] Train Epoch: 0\t[136/3394 (13600%)]\tLoss: 0.595705, TimeAvg: 0.3552, Remaining: 00:39:22\n",
      "  [46.887s] Train Epoch: 0\t[137/3394 (13700%)]\tLoss: 0.620592, TimeAvg: 0.3552, Remaining: 00:39:22\n",
      "  [47.234s] Train Epoch: 0\t[138/3394 (13800%)]\tLoss: 0.570172, TimeAvg: 0.3551, Remaining: 00:39:21\n",
      "  [47.566s] Train Epoch: 0\t[139/3394 (13900%)]\tLoss: 0.533929, TimeAvg: 0.3549, Remaining: 00:39:19\n",
      "  [47.904s] Train Epoch: 0\t[140/3394 (14000%)]\tLoss: 0.548585, TimeAvg: 0.3548, Remaining: 00:39:18\n",
      "  [48.267s] Train Epoch: 0\t[141/3394 (14100%)]\tLoss: 0.674302, TimeAvg: 0.3548, Remaining: 00:39:18\n",
      "  [48.663s] Train Epoch: 0\t[142/3394 (14200%)]\tLoss: 0.813718, TimeAvg: 0.3551, Remaining: 00:39:20\n",
      "  [49.018s] Train Epoch: 0\t[143/3394 (14300%)]\tLoss: 0.871280, TimeAvg: 0.3551, Remaining: 00:39:19\n",
      "  [49.351s] Train Epoch: 0\t[144/3394 (14400%)]\tLoss: 0.895095, TimeAvg: 0.3550, Remaining: 00:39:18\n",
      "  [49.697s] Train Epoch: 0\t[145/3394 (14500%)]\tLoss: 0.951975, TimeAvg: 0.3549, Remaining: 00:39:17\n",
      "  [50.044s] Train Epoch: 0\t[146/3394 (14600%)]\tLoss: 1.130975, TimeAvg: 0.3548, Remaining: 00:39:16\n",
      "  [50.407s] Train Epoch: 0\t[147/3394 (14700%)]\tLoss: 1.539009, TimeAvg: 0.3549, Remaining: 00:39:16\n",
      "  [50.792s] Train Epoch: 0\t[148/3394 (14800%)]\tLoss: 2.037226, TimeAvg: 0.3551, Remaining: 00:39:17\n",
      "  [51.167s] Train Epoch: 0\t[149/3394 (14900%)]\tLoss: 2.188374, TimeAvg: 0.3553, Remaining: 00:39:18\n",
      "  [51.514s] Train Epoch: 0\t[150/3394 (15000%)]\tLoss: 2.094495, TimeAvg: 0.3552, Remaining: 00:39:17\n",
      "  [51.851s] Train Epoch: 0\t[151/3394 (15100%)]\tLoss: 2.065855, TimeAvg: 0.3551, Remaining: 00:39:16\n",
      "  [52.219s] Train Epoch: 0\t[152/3394 (15200%)]\tLoss: 2.045415, TimeAvg: 0.3552, Remaining: 00:39:16\n",
      "  [52.588s] Train Epoch: 0\t[153/3394 (15300%)]\tLoss: 2.035851, TimeAvg: 0.3553, Remaining: 00:39:17\n",
      "  [53.002s] Train Epoch: 0\t[154/3394 (15400%)]\tLoss: 2.038949, TimeAvg: 0.3556, Remaining: 00:39:19\n",
      "  [53.347s] Train Epoch: 0\t[155/3394 (15500%)]\tLoss: 2.003495, TimeAvg: 0.3556, Remaining: 00:39:18\n",
      "  [53.687s] Train Epoch: 0\t[156/3394 (15600%)]\tLoss: 1.980478, TimeAvg: 0.3555, Remaining: 00:39:17\n",
      "  [54.070s] Train Epoch: 0\t[157/3394 (15700%)]\tLoss: 1.958044, TimeAvg: 0.3557, Remaining: 00:39:18\n",
      "  [54.445s] Train Epoch: 0\t[158/3394 (15800%)]\tLoss: 1.957891, TimeAvg: 0.3558, Remaining: 00:39:18\n",
      "  [54.785s] Train Epoch: 0\t[159/3394 (15900%)]\tLoss: 1.969184, TimeAvg: 0.3557, Remaining: 00:39:17\n",
      "  [55.126s] Train Epoch: 0\t[160/3394 (16000%)]\tLoss: 1.917865, TimeAvg: 0.3556, Remaining: 00:39:16\n",
      "  [55.481s] Train Epoch: 0\t[161/3394 (16100%)]\tLoss: 1.886754, TimeAvg: 0.3555, Remaining: 00:39:16\n",
      "  [55.860s] Train Epoch: 0\t[162/3394 (16200%)]\tLoss: 1.867890, TimeAvg: 0.3557, Remaining: 00:39:16\n",
      "  [56.233s] Train Epoch: 0\t[163/3394 (16300%)]\tLoss: 1.845923, TimeAvg: 0.3558, Remaining: 00:39:17\n",
      "  [56.582s] Train Epoch: 0\t[164/3394 (16400%)]\tLoss: 1.058587, TimeAvg: 0.3558, Remaining: 00:39:16\n",
      "  [56.914s] Train Epoch: 0\t[165/3394 (16500%)]\tLoss: 1.793010, TimeAvg: 0.3556, Remaining: 00:39:15\n",
      "  [57.254s] Train Epoch: 0\t[166/3394 (16600%)]\tLoss: 1.776835, TimeAvg: 0.3556, Remaining: 00:39:14\n",
      "  [57.641s] Train Epoch: 0\t[167/3394 (16700%)]\tLoss: 1.750843, TimeAvg: 0.3557, Remaining: 00:39:15\n",
      "  [58.019s] Train Epoch: 0\t[168/3394 (16800%)]\tLoss: 1.722290, TimeAvg: 0.3559, Remaining: 00:39:15\n",
      "  [58.359s] Train Epoch: 0\t[169/3394 (16900%)]\tLoss: 1.690274, TimeAvg: 0.3558, Remaining: 00:39:14\n",
      "  [58.697s] Train Epoch: 0\t[170/3394 (17000%)]\tLoss: 1.652461, TimeAvg: 0.3557, Remaining: 00:39:13\n",
      "  [59.071s] Train Epoch: 0\t[171/3394 (17100%)]\tLoss: 1.604611, TimeAvg: 0.3558, Remaining: 00:39:14\n",
      "  [59.440s] Train Epoch: 0\t[172/3394 (17200%)]\tLoss: 1.552830, TimeAvg: 0.3558, Remaining: 00:39:14\n",
      "  [59.773s] Train Epoch: 0\t[173/3394 (17300%)]\tLoss: 1.493817, TimeAvg: 0.3557, Remaining: 00:39:13\n",
      "  [1.002m] Train Epoch: 0\t[174/3394 (17400%)]\tLoss: 1.444804, TimeAvg: 0.3556, Remaining: 00:39:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [1.008m] Train Epoch: 0\t[175/3394 (17500%)]\tLoss: 1.398062, TimeAvg: 0.3555, Remaining: 00:39:11\n",
      "  [1.014m] Train Epoch: 0\t[176/3394 (17600%)]\tLoss: 1.357893, TimeAvg: 0.3557, Remaining: 00:39:12\n",
      "  [1.020m] Train Epoch: 0\t[177/3394 (17700%)]\tLoss: 1.352181, TimeAvg: 0.3559, Remaining: 00:39:12\n",
      "  [1.026m] Train Epoch: 0\t[178/3394 (17800%)]\tLoss: 1.328584, TimeAvg: 0.3559, Remaining: 00:39:12\n",
      "  [1.033m] Train Epoch: 0\t[179/3394 (17900%)]\tLoss: 1.315452, TimeAvg: 0.3561, Remaining: 00:39:13\n",
      "  [1.039m] Train Epoch: 0\t[180/3394 (18000%)]\tLoss: 1.256762, TimeAvg: 0.3562, Remaining: 00:39:13\n",
      "  [1.045m] Train Epoch: 0\t[181/3394 (18100%)]\tLoss: 1.210463, TimeAvg: 0.3563, Remaining: 00:39:14\n",
      "  [1.051m] Train Epoch: 0\t[182/3394 (18200%)]\tLoss: 1.173370, TimeAvg: 0.3562, Remaining: 00:39:13\n",
      "  [1.058m] Train Epoch: 0\t[183/3394 (18300%)]\tLoss: 1.138433, TimeAvg: 0.3565, Remaining: 00:39:14\n",
      "  [1.064m] Train Epoch: 0\t[184/3394 (18400%)]\tLoss: 1.095427, TimeAvg: 0.3566, Remaining: 00:39:14\n",
      "  [1.070m] Train Epoch: 0\t[185/3394 (18500%)]\tLoss: 1.089098, TimeAvg: 0.3566, Remaining: 00:39:14\n",
      "  [1.076m] Train Epoch: 0\t[186/3394 (18600%)]\tLoss: 0.991125, TimeAvg: 0.3566, Remaining: 00:39:13\n",
      "  [1.082m] Train Epoch: 0\t[187/3394 (18700%)]\tLoss: 1.211225, TimeAvg: 0.3566, Remaining: 00:39:13\n",
      "  [1.088m] Train Epoch: 0\t[188/3394 (18800%)]\tLoss: 0.868229, TimeAvg: 0.3566, Remaining: 00:39:13\n",
      "  [1.094m] Train Epoch: 0\t[189/3394 (18900%)]\tLoss: 0.818604, TimeAvg: 0.3565, Remaining: 00:39:12\n",
      "  [1.099m] Train Epoch: 0\t[190/3394 (19000%)]\tLoss: 0.712553, TimeAvg: 0.3565, Remaining: 00:39:12\n",
      "  [1.105m] Train Epoch: 0\t[191/3394 (19100%)]\tLoss: 0.637887, TimeAvg: 0.3564, Remaining: 00:39:11\n",
      "  [1.111m] Train Epoch: 0\t[192/3394 (19200%)]\tLoss: 0.643845, TimeAvg: 0.3564, Remaining: 00:39:10\n",
      "  [1.117m] Train Epoch: 0\t[193/3394 (19300%)]\tLoss: 0.504878, TimeAvg: 0.3564, Remaining: 00:39:10\n",
      "  [1.123m] Train Epoch: 0\t[194/3394 (19400%)]\tLoss: 0.420483, TimeAvg: 0.3563, Remaining: 00:39:09\n",
      "  [1.128m] Train Epoch: 0\t[195/3394 (19500%)]\tLoss: 0.232285, TimeAvg: 0.3562, Remaining: 00:39:08\n",
      "  [1.134m] Train Epoch: 0\t[196/3394 (19600%)]\tLoss: 0.231651, TimeAvg: 0.3561, Remaining: 00:39:07\n",
      "  [1.139m] Train Epoch: 0\t[197/3394 (19700%)]\tLoss: 0.228624, TimeAvg: 0.3560, Remaining: 00:39:06\n",
      "  [1.145m] Train Epoch: 0\t[198/3394 (19800%)]\tLoss: 0.338448, TimeAvg: 0.3560, Remaining: 00:39:06\n",
      "  [1.151m] Train Epoch: 0\t[199/3394 (19900%)]\tLoss: 0.438393, TimeAvg: 0.3560, Remaining: 00:39:05\n",
      "  [1.157m] Train Epoch: 0\t[200/3394 (20000%)]\tLoss: 0.449175, TimeAvg: 0.3558, Remaining: 00:39:04\n",
      "  [1.162m] Train Epoch: 0\t[201/3394 (20100%)]\tLoss: 0.376625, TimeAvg: 0.3557, Remaining: 00:39:03\n",
      "  [1.168m] Train Epoch: 0\t[202/3394 (20200%)]\tLoss: 0.269162, TimeAvg: 0.3556, Remaining: 00:39:02\n",
      "  [1.173m] Train Epoch: 0\t[203/3394 (20300%)]\tLoss: 0.226868, TimeAvg: 0.3555, Remaining: 00:39:01\n",
      "  [1.179m] Train Epoch: 0\t[204/3394 (20400%)]\tLoss: 0.211452, TimeAvg: 0.3555, Remaining: 00:39:00\n",
      "  [1.185m] Train Epoch: 0\t[205/3394 (20500%)]\tLoss: 0.235986, TimeAvg: 0.3554, Remaining: 00:38:59\n",
      "  [1.190m] Train Epoch: 0\t[206/3394 (20600%)]\tLoss: 0.219475, TimeAvg: 0.3553, Remaining: 00:38:58\n",
      "  [1.191m] Operation failed\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-b5a9b4a2e52f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-30feeee2f782>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, imu)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflownet_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[2, 128, 112] # [1, 2, 8, 7]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;31m#         print('c_out', c_out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/other people's repo/flownet2-pytorch/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# flownetc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mflownetc_flow2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflownetc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mflownetc_flow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflownetc_flow2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/other people's repo/flownet2-pytorch/networks/FlowNetC.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mflow6\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_flow6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_conv6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mflow6_up\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsampled_flow6_to_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mout_deconv5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_conv6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('creating model.................')\n",
    "net = FlowNet2(args).cuda()\n",
    "print('loading weight.................')\n",
    "state_dict = torch.load(\"FlowNet2_checkpoint.pth.tar\")\n",
    "net.load_state_dict(state_dict[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pim1 = read_gen(\"/home/projectx/Downloads/training/clean/ambush_5/frame_0037.png\")\n",
    "# pim2 = read_gen(\"/home/projectx/Downloads/training/clean/ambush_5/frame_0038.png\")\n",
    "pim1 = read_gen(\"/home/projectx/Documents/GitHub repos/VINET_modification/Train_1/caolin_frame0822.jpg\")\n",
    "pim2 = read_gen(\"/home/projectx/Documents/GitHub repos/VINET_modification/Train_1/caolin_frame0824.jpg\")\n",
    "\n",
    "images = [pim1[:,:], pim2[:,:]]\n",
    "images = np.array(images).transpose(3, 0, 1, 2)\n",
    "im = torch.from_numpy(images.astype(np.float32)).unsqueeze(0).cuda()\n",
    "print(im.size())\n",
    "\n",
    "plt.imshow(pim1)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(pim2)\n",
    "plt.show()\n",
    "\n",
    "result = net(im).squeeze()\n",
    "\n",
    "result.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = result.data.cpu().numpy().transpose(1, 2, 0)\n",
    "# flow.shape\n",
    "\n",
    "image = flow_to_image(flow)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Surgical_VINet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.flownet_c(im).squeeze()\n",
    "result.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(X,X2)\n",
    "(output.size(), Y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss(size_average=False)\n",
    "\n",
    "loss = criterion(output, Y)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
